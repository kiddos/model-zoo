{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cc05f4-56fd-4292-b070-3d0d1a958832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-26 16:16:32.827337: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers 4.26.0\n",
      "pytorch 1.13.0a0+git49444c3\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import default_data_collator\n",
    "from transformers import pipeline\n",
    "from transformers import get_scheduler\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "print('transformers', transformers.__version__)\n",
    "print('pytorch', torch.version.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05b3ff3-5ab3-4256-98d6-179e2c569c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperParameters:\n",
    "    chunk_size = 128\n",
    "    model_name = 'distilbert-base-uncased'\n",
    "    wwm_probability = 0.2\n",
    "    mlm_probability = 0.15\n",
    "    batch_size = 32\n",
    "    learning_rate = 5e-5\n",
    "    epochs = 6\n",
    "\n",
    "params = HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d1d3378-3bac-4ca3-bc86-36edf68e71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(params.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b83d283-a9bb-4e63-b6a8-6e392ffdc085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> DistilBERT number of parameters: 67M'\n",
      "'>>> BERT number of parameters: 110M'\n"
     ]
    }
   ],
   "source": [
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\")\n",
    "print(f\"'>>> BERT number of parameters: 110M'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61740b0d-f483-4fe4-aaba-02d324543f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> This is a great deal.'\n",
      "'>>> This is a great success.'\n",
      "'>>> This is a great adventure.'\n",
      "'>>> This is a great idea.'\n",
      "'>>> This is a great feat.'\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(params.model_name)\n",
    "\n",
    "text = 'This is a great [MASK].'\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd6609b-f74c-4c06-afcb-589f04efdfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset imdb (/home/kiddos/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0149688720703125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776046414c79400a9c046f64c0158467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9a84a52-336f-45bf-a019-cc2ba88ca887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Review: I saw this film at a pre-release screening at the Writers Guild theater in Beverly Hills. As I recall, the film's producers and director were in attendance, presumably to gage our reaction.<br /><br />Many scenes evoked gales of laughter from the audience, which would have been fine if it had been a comedy, but it was supposed to be a horror film.<br /><br />If the audience wasn't scared, it seems the filmmakers were. They delayed release for over a year. Out of curiosity I saw it again to see if they'd re-cut it; as far as I can tell, they hadn't. It was the same lousy movie, just a year older.<br /><br />It almost qualifies as \"so bad, it's good,\" but it's slow-paced and boring.\n",
      ">>> Label: 0\n",
      "\n",
      ">>> Review: I had a personal interest in this movie. When I was 17 and just out of high school I got a job at 20th Century Fox as a member of the Laborers and Hod Carriers Union. At the end of my first day (sweeping the deck of an aircraft carrier) I was told to bring a suitcase the next morning with enough clothes etc. for one or two weeks. When I arrived the next morning a bus was waiting and about 20 of us headed south toward San Diego. Just short of there we stopped at an army base called either Camp Callan or Camp Hahn. Once we were bunked in we went north a few miles into Camp Pendleton, the big Marine base. There, on the beach, we started building what was supposed to be a Japanese Pacific island base. It took us about a week or ten days to complete the installation, which included a water tank, gun entrenchments, sand-bagged trenches and living quarters. All this was at very high pay, sometimes 'golden time', which was triple our regular hourly wage. Our food was also first rate = prime rib at lunch, etc. - which was amazing because it was wartime and very hard to get good meat at home.<br /><br />Once the job was finished I waited eagerly for the movie to come out, which was about eight or ten months later. Then I waited eagerly through two hours of the movie before my handiwork finally came on screen. Then it was no more than three or four minutes (maybe less) of the movie's heroes dive bombing the base and blowing it to smithereens. A bit disappointing, but still fun. <br /><br />In spite of the disappointment I enjoyed the movie and have not seen it since. I learned later that this movie was underwritten by the government and Fox was paid on a cost plus basis, which maybe accounts for our extravagant pay and lifestyle down there. Bob Weverka\n",
      ">>> Label: 1\n",
      "\n",
      ">>> Review: Not sure why it doesn't play in Peoria, apparently, but this is a very funny, clever British comedy. It's set at the end of the \"swinging sixties\". Peter Sellars is fantastic as the rich, forty-something serial womaniser. The perfectly delectable Goldie Hawn, playing a 19 year American girl in London, is, initially, Sellars' \"catch of the day\". But the urbane TV food critic can't stop himself from falling for the dizzy American blond.<br /><br />Humour, pathos, great script, strong performances from the leads and supporting caste.<br /><br />It's a great film, and the best gag is the very last line.<br /><br />Try it, you'll like it.\n",
      ">>> Label: 1\n"
     ]
    }
   ],
   "source": [
    "sample = imdb_dataset['train'].shuffle().select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n>>> Review: {row['text']}\")\n",
    "    print(f\">>> Label: {row['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e462d82f-5212-4353-a2b0-894dd6ff9a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/kiddos/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-b8f81710835e208f.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/kiddos/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-2d976eaef8830af9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/kiddos/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-f97b59f97b076841.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples['text'])\n",
    "    if tokenizer.is_fast:\n",
    "        result['word_ids'] = [result.word_ids(i) for i in range(len(result['input_ids']))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=['text', 'label']\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bd5073d-de43-4d5b-bfd7-29114879aea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ecfa687-fb20-4d25-9f50-c1428a527ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Review 0 length: 363\n",
      ">>> Review 1 length: 304\n",
      ">>> Review 2 length: 133\n"
     ]
    }
   ],
   "source": [
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_datasets['train'][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples['input_ids']):\n",
    "    print(f'>>> Review {idx} length: {len(sample)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7feecf9-dcec-46fd-8f96-a96f188cd99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated reviews length: 800'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97a5af70-f54f-40d7-8ef7-54bc0e2b82cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([len(sample) for sample in tokenized_datasets['train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "856bea2d-e8a0-4a85-b8e3-ab9bd481e42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n"
     ]
    }
   ],
   "source": [
    "total_length = ((total_length + params.chunk_size -1) // params.chunk_size) * params.chunk_size\n",
    "\n",
    "chunks = {}\n",
    "for k, t in concatenated_examples.items():\n",
    "    t = np.pad(t, (0, total_length - len(t)))\n",
    "    chunks[k] = [t[i : i + params.chunk_size] for i in range(0, total_length, params.chunk_size)]\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa356fbf-9ed4-49e4-abb5-83d0e6a8dd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010535001754760742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 25,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a60eafcdea4b99b21e6eccac4f6a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013719320297241211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 25,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2455e6a19b4debba0e957400f97ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009826898574829102,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 50,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725339ae270346a794091c76eefeb4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 61314\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 59929\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 123007\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = ((total_length + params.chunk_size -1) // params.chunk_size) * params.chunk_size\n",
    "    result = {}\n",
    "    for k, t in concatenated_examples.items():\n",
    "        t = np.pad(t, (0, total_length - len(t)))\n",
    "        result[k] = [t[i : i + params.chunk_size] for i in range(0, total_length, params.chunk_size)]\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92000b57-a9bd-4632-aca0-50d8628bb670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets['train'][1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf1490e9-913f-4630-8537-c649c8a4b194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets['train'][1]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68149abf-812d-4b97-8cd2-e724da7a8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=params.mlm_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2053cad1-0631-4e51-b77e-6ae7a2909ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CLS] i rented [MASK] am [MASK] - yellow from my video store because of all the controversy that surrounded it when it was [MASK] released in 1967. [MASK] also heard that at [MASK] it was seized by u [MASK] s. customs if it ever tried to enter this country [MASK] [MASK] [MASK] a fan of [MASK] considered \" controversial \" i really had to see this for myself [MASK] [MASK] br / > < br / > the plot is centered around a young swedish drama student [MASK] lena who wants [MASK] learn everything she can [MASK] life. fuels particular she wants [MASK] focus her [MASK]s to making some sort of documentary on what [MASK] average swede thought about certain political issues such\n",
      "\n",
      ">>> as the vietnam [MASK] and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex [MASK] [MASK] drama teacher, classmates, and [MASK] men. < [MASK] / > < br / > what kills me about i am curious - yellow [MASK] that 40 years [MASK], this was considered pornographic. really, the sex and nudity scenes [MASK] few and far between, even then it's not shot like some cheaply made porno. while my country [MASK] mind [MASK] it shocking, in reality sex and nudity are [MASK] major staple in swedish cinema. even ingmar bergman,\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets['train'][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop('word_ids')\n",
    "\n",
    "for chunk in data_collator(samples)['input_ids']:\n",
    "    print(f'\\n>>> {tokenizer.decode(chunk)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2380f35-1aa7-43e2-a11b-0a00feeef0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] i rented [MASK] am curious - [MASK] [MASK] my [MASK] store [MASK] [MASK] all the controversy that [MASK] it when it was first released in 1967. i also [MASK] that [MASK] first it was seized [MASK] u. s [MASK] customs [MASK] it ever [MASK] [MASK] enter this country [MASK] therefore being a fan [MASK] [MASK] considered [MASK] controversial \" [MASK] really had to [MASK] this for myself. < br / > < br / > the plot [MASK] centered around a young swedish [MASK] student named lena [MASK] wants to learn everything she can about life [MASK] in [MASK] she [MASK] to [MASK] her [MASK] [MASK] to making some sort of documentary on what the [MASK] swede thought about certain political issues such'\n",
      "\n",
      "'>>> [MASK] the [MASK] war and [MASK] issues in the united states. in [MASK] [MASK] politicians and ordinary [MASK] [MASK] [MASK] of stockholm about [MASK] opinions on politics, she has sex [MASK] her drama teacher, classmates, and married men. [MASK] br / > < [MASK] / [MASK] what kills me about i am curious - yellow is [MASK] 40 years ago, this was [MASK] [MASK] [MASK] really, the sex and nudity scenes are few and far between, even then it's not shot [MASK] [MASK] cheaply made [MASK] [MASK]. while my countrymen mind [MASK] it shocking [MASK] in reality sex [MASK] [MASK] [MASK] are a major staple in swedish [MASK]. [MASK] ingmar bergman [MASK]'\n"
     ]
    }
   ],
   "source": [
    "# if we want to mask the entire word instead of just single token, we have to implement this function\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop('word_ids')\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, params.wwm_probability, (len(mapping),))\n",
    "        input_ids = feature['input_ids']\n",
    "        labels = feature['labels']\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature['labels'] = new_labels\n",
    "\n",
    "    return default_data_collator(features)\n",
    "\n",
    "\n",
    "samples = [lm_datasets['train'][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch['input_ids']:\n",
    "    print(f'\\n>>> {tokenizer.decode(chunk)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2ca5347-93b0-49d0-bfb5-9b4c6d3abc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 55182\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 6132\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size = len(lm_datasets['train'])\n",
    "train_size = int(data_size * 0.9)\n",
    "test_size = data_size - train_size\n",
    "\n",
    "downsampled_dataset = lm_datasets['train'].train_test_split(\n",
    "    train_size=train_size, test_size=test_size,\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a091b34-d654-45d3-ab65-93e04f6b1d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010622501373291016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0531392c552248daa17419aca6fcdbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {'masked_' + k: v.numpy() for k, v in masked_inputs.items()}\n",
    "\n",
    "\n",
    "downsampled_dataset = downsampled_dataset.remove_columns(['word_ids'])\n",
    "eval_dataset = downsampled_dataset['test'].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=downsampled_dataset['test'].column_names,\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        'masked_input_ids': 'input_ids',\n",
    "        'masked_attention_mask': 'attention_mask',\n",
    "        'masked_labels': 'labels',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76b1324a-17d3-4937-9f9b-7db3aa9b8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    downsampled_dataset['train'],\n",
    "    shuffle=True,\n",
    "    batch_size=params.batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=params.batch_size, collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4bc3628-108b-4d92-a3af-01dd114c239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "optimizer = AdamW(model.parameters(), lr=params.learning_rate)\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55f1d6f8-ef6c-405f-8fcc-c3d9a1af7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = params.epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e60c57a-dc5e-4e5f-af31-01e15cc8e390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011070489883422852,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10350,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a5a812254b48f6a7908ae6900bcad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0: Perplexity: 9.057567596435547\n",
      ">>> Epoch 1: Perplexity: 8.754276275634766\n",
      ">>> Epoch 2: Perplexity: 8.54840087890625\n",
      ">>> Epoch 3: Perplexity: 8.4483060836792\n",
      ">>> Epoch 4: Perplexity: 8.4483060836792\n",
      ">>> Epoch 5: Perplexity: 8.4483060836792\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "output_dir = f'{params.model_name}-finetuned-imdb-accelerate'\n",
    "\n",
    "for epoch in range(params.epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(params.batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = torch.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61f55daa-e033-4146-8e29-8649be21c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "fill_masker = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a33985c-7951-4494-80af-ad6f7d052217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.43710577487945557,\n",
       "  'token': 3185,\n",
       "  'token_str': 'movie',\n",
       "  'sequence': 'this is a great movie.'},\n",
       " {'score': 0.31932228803634644,\n",
       "  'token': 2143,\n",
       "  'token_str': 'film',\n",
       "  'sequence': 'this is a great film.'},\n",
       " {'score': 0.017497606575489044,\n",
       "  'token': 4038,\n",
       "  'token_str': 'comedy',\n",
       "  'sequence': 'this is a great comedy.'},\n",
       " {'score': 0.017465347424149513,\n",
       "  'token': 2265,\n",
       "  'token_str': 'show',\n",
       "  'sequence': 'this is a great show.'},\n",
       " {'score': 0.013669433072209358,\n",
       "  'token': 2801,\n",
       "  'token_str': 'idea',\n",
       "  'sequence': 'this is a great idea.'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_masker('This is a great [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da21d32-ae39-4bf0-bf17-7098d5b2d944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
