{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cc05f4-56fd-4292-b070-3d0d1a958832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 10:50:56.992963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers 4.26.0\n",
      "pytorch 1.13.0a0+git49444c3\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import default_data_collator\n",
    "from transformers import pipeline\n",
    "from transformers import get_scheduler\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "print('transformers', transformers.__version__)\n",
    "print('pytorch', torch.version.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05b3ff3-5ab3-4256-98d6-179e2c569c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperParameters:\n",
    "    chunk_size = 128\n",
    "    model_name = 'distilbert-base-uncased'\n",
    "    wwm_probability = 0.2\n",
    "    mlm_probability = 0.15\n",
    "    batch_size = 32\n",
    "    learning_rate = 5e-5\n",
    "    epochs = 6\n",
    "\n",
    "\n",
    "params = HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d1d3378-3bac-4ca3-bc86-36edf68e71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(params.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b83d283-a9bb-4e63-b6a8-6e392ffdc085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> DistilBERT number of parameters: 67M'\n",
      "'>>> BERT number of parameters: 110M'\n"
     ]
    }
   ],
   "source": [
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\")\n",
    "print(f\"'>>> BERT number of parameters: 110M'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61740b0d-f483-4fe4-aaba-02d324543f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> This is a great deal.'\n",
      "'>>> This is a great success.'\n",
      "'>>> This is a great adventure.'\n",
      "'>>> This is a great idea.'\n",
      "'>>> This is a great feat.'\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(params.model_name)\n",
    "\n",
    "text = 'This is a great [MASK].'\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd6609b-f74c-4c06-afcb-589f04efdfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset imdb (/home/kiddos/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014143705368041992,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222421c0498d4718b8419d240a9402d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9a84a52-336f-45bf-a019-cc2ba88ca887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Review: Now i have read some negative reviews for this show on this website and quite frankly I'm appalled. For anyone to even think that the Sopranos is not Television then i'm afraid i don't know what the world has come to. Let me tell u something. I started watching many T.V shows like Lost, Prison Break, Dexter, Deadwood and even Invasion. But all of those shows lost their touch after the first season, especially Lost and Prison Break which i refuse to watch because the companies took 2 genius ideas and butchered them by making more than one season. Then we have The Sopranos. I can honestly say that this is the only television series that i have ever watched where i have been enthralled in all of its season, and more importantly all of its episodes. There is no department that this show doesn't excel in. Acting- Nothing short of superb. James Gandolfini is one of my favourite actors and i feel that his acting is absolutely stunning in every episode, after i heard that HBO wanted Ray Liotta to play Tony i felt that it would've been the better choice, however after watching the first few episodes, i knew that HBO had done a great job in casting James as Tony. The raw emotion he displays is superb. Then we have everyone else, Edie Falco, Michael Imperioli, Lorraine Bracco, Dominic Chianese (whom i remembered as Johnny Ola in the Godfather Part 2) and my personal two favourite characters Tony Sirico and Steve Van Zandt Paulie 'Walnuts' Gualtieri and Silvio Dante. All of these actors perform to the best quality, and all giving an excellent performance in each episode. Then we have the story, never have i been so sucked into a T.V show before. The story is nothing short of excellent. Each episode is directed superbly and the Score of this show is just fantastic. I feel that The Sopranos is one show that i can watch again and again and never get bored of. Its got everything from hilarious humour to brutal violence, but nonetheless it is and will always be the best thing to ever grace the Television, and I challenge anyone to find a real flaw in the show. Not just say its too violent, or they feel that the character of Tony is immoral, i mean it is a mafia show at the end of the day, i don't think that the characters are going to be very honest or loyal to God. I implore everyone to watch this show because believe me, you'll be hooked from the very first episode, i was and i have even gotten a few friends who had firstly refused to watch the show, hooked on it. Trust me when i say that this show is a Godsend compared to the crap that comes on T.V. After you've watched the first season, you'll inevitably agree with me when i once again say that this show dominates Television, and no T.V show current or future will ever upstage the marvel that is The Sopranos.\n",
      ">>> Label: 1\n",
      "\n",
      ">>> Review: Notorious for more than a quarter century (and often banned), it's obscurity was its greatest asset it seems. Hey, it's often better to be talked about, rather than actually seen when you can't back the \"legend\" up with substance.<br /><br />The film has played in Los Angeles a couple of times recently, and is available on home video, so that veil is slowly being lifted. While there is still plenty to offend the masses, it is more likely to bore them, than arouse much real passion. Except for a gratuitous and protracted XXX sex scene between a pair of horses (\"Nature Documentary\" anyone?), there follows nearly an hour of a dull arranged marriage melodrama. <br /><br />Once the sex and nudity begins, it is a nonstop sequence involving masturbation, a looooooooong flashback to an alleged 'beauty and the beast' encounter, and a naked woman running around the mansion (nobody, even her supposedly protective Aunt, seems to even think of putting some clothes on her!). On video, I guess you can fast-forward thru the banality, but it's not really worth the effort. The nudity doesn't go beyond what is seen in something much more substantive such as Bertolucci's THE DREAMERS.<br /><br />Try as one might to find some 'moral' or 'symbolism' in the carnality, I doubt it's worthy of anyone's effort. Unfortunately, for LA BETE, now that you can more easily see the film, the notoriety of something once 'forbidden' has been lifted. And this beast has been tamed.\n",
      ">>> Label: 0\n",
      "\n",
      ">>> Review: This is not my favorite WIP (\"Women in Prison\"), but it is one of the most famous films in the sub-genre. It is was produced by Roger Corman, who at this point had already produced a few WIPs. It is obvious that the film tries to play with the established formula. The movie takes place in an USA prison, not in a \"banana republic\" like most WIP films. I'm not sure if that was a wise move, but it is an acceptable change of pace. Writer-director Demme really gets into his job, always digging for new ways to present a familiar scenario. In fact, he is a little too ambitious for his own good. The filmmaker creates a few surreal dream sequences that are borderline pretentious but it is fun to see how hard he tries to put this film above your average chicks-in-chains flick. But do not worry, Demme still operates within the parameters of the sub-genre. There is plenty of nudity and violence, something that will satisfy hardcore fans. The film is a little slow, but it is very entertaining. The cast is good. Roberta Collins is a WIP veteran, so she does not need an introduction, and Barbara Steel is a hoot as the wheelchair-bound crazy warden. Pam Grier is sorely missed, though.\n",
      ">>> Label: 1\n"
     ]
    }
   ],
   "source": [
    "sample = imdb_dataset['train'].shuffle().select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n>>> Review: {row['text']}\")\n",
    "    print(f\">>> Label: {row['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e462d82f-5212-4353-a2b0-894dd6ff9a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/kiddos/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-b8f81710835e208f.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/kiddos/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-2d976eaef8830af9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/kiddos/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-f97b59f97b076841.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples['text'])\n",
    "    if tokenizer.is_fast:\n",
    "        result['word_ids'] = [result.word_ids(i) for i in range(len(result['input_ids']))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=['text', 'label']\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bd5073d-de43-4d5b-bfd7-29114879aea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ecfa687-fb20-4d25-9f50-c1428a527ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Review 0 length: 363\n",
      ">>> Review 1 length: 304\n",
      ">>> Review 2 length: 133\n"
     ]
    }
   ],
   "source": [
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_datasets['train'][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples['input_ids']):\n",
    "    print(f'>>> Review {idx} length: {len(sample)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7feecf9-dcec-46fd-8f96-a96f188cd99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated reviews length: 800'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97a5af70-f54f-40d7-8ef7-54bc0e2b82cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([len(sample) for sample in tokenized_datasets['train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "856bea2d-e8a0-4a85-b8e3-ab9bd481e42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n"
     ]
    }
   ],
   "source": [
    "total_length = ((total_length + params.chunk_size -1) // params.chunk_size) * params.chunk_size\n",
    "\n",
    "chunks = {}\n",
    "for k, t in concatenated_examples.items():\n",
    "    t = np.pad(t, (0, total_length - len(t)))\n",
    "    chunks[k] = [t[i : i + params.chunk_size] for i in range(0, total_length, params.chunk_size)]\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa356fbf-9ed4-49e4-abb5-83d0e6a8dd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/kiddos/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-4edc79105b45b270.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/kiddos/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-90aa5673ea746979.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/kiddos/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-98ebc0bfc2a1a61b.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 61314\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 59929\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 123007\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = ((total_length + params.chunk_size -1) // params.chunk_size) * params.chunk_size\n",
    "    result = {}\n",
    "    for k, t in concatenated_examples.items():\n",
    "        t = np.pad(t, (0, total_length - len(t)))\n",
    "        result[k] = [t[i : i + params.chunk_size] for i in range(0, total_length, params.chunk_size)]\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92000b57-a9bd-4632-aca0-50d8628bb670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets['train'][1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf1490e9-913f-4630-8537-c649c8a4b194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets['train'][1]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68149abf-812d-4b97-8cd2-e724da7a8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=params.mlm_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2053cad1-0631-4e51-b77e-6ae7a2909ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CLS] i rented i am curious - yellow from [MASK] video store because of [MASK] the controversy that surrounded it when it was first [MASK] [MASK] 1967. i also heard that at first it was seized by u. s. customs if [MASK] ever tried to enter this country, therefore being [MASK] fan of films considered \" controversial [MASK] [MASK] really had to see this for myself. < br / > < [MASK] / > the plot is centered around a young swedish [MASK] student named lena who wants to learn everything she [MASK] about life. in particular she wants to [MASK] [MASK] attentions [MASK] making some sort of documentary on [MASK] the average swede thought about certain political issues such\n",
      "\n",
      ">>> as the vietnamount and [MASK] issues in the united states. in [MASK] asking politicians [MASK] ordinary den [MASK]ns ofnist about their [MASK] on politics, she has sex with [MASK] drama teacher, classmates, and married men. < br / > < br / > what kills me about i am [MASK] [MASK] yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are [MASK] and far between, even [MASK] it's not [MASK] like [MASK] cheaply made porno [MASK] while my countrymen mind find it [MASK], in reality [MASK] and nudity [MASK] a major staple in swedish cinema [MASK] even ingmar bergman,\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets['train'][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop('word_ids')\n",
    "\n",
    "for chunk in data_collator(samples)['input_ids']:\n",
    "    print(f'\\n>>> {tokenizer.decode(chunk)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2380f35-1aa7-43e2-a11b-0a00feeef0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CLS] i rented [MASK] [MASK] [MASK] - yellow from my [MASK] [MASK] [MASK] [MASK] all the controversy that surrounded [MASK] [MASK] it was first released [MASK] 1967. [MASK] also heard that [MASK] first it was [MASK] by [MASK]. s. [MASK] if it ever tried to enter this country, therefore being a fan of films [MASK] \" controversial \" i really had to [MASK] this for myself [MASK] < br [MASK] > < br / > the [MASK] is centered around a young swedish [MASK] student named lena who wants to learn everything she can about life. in particular she wants to focus [MASK] attentions to making some [MASK] of documentary on what [MASK] [MASK] swede thought [MASK] certain political issues such\n",
      "\n",
      ">>> as [MASK] vietnam war [MASK] [MASK] issues [MASK] the united states. in [MASK] asking [MASK] and [MASK] denizens [MASK] stockholm about their [MASK] on politics, she has sex with her [MASK] teacher, classmates, and married men. < br / > < br / > what [MASK] me about [MASK] am curious - yellow is that 40 years ago, this was considered pornographic. [MASK], the sex and nudity scenes are few and far between, even then it's not shot like [MASK] cheaply made porno [MASK] [MASK] my countrymen [MASK] find it shocking, in reality [MASK] and [MASK] [MASK] are a [MASK] [MASK] in swedish cinema [MASK] even [MASK] [MASK] bergman,\n"
     ]
    }
   ],
   "source": [
    "# if we want to mask the entire word instead of just single token, we have to implement this function\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop('word_ids')\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, params.wwm_probability, (len(mapping),))\n",
    "        input_ids = feature['input_ids']\n",
    "        labels = feature['labels']\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature['labels'] = new_labels\n",
    "\n",
    "    return default_data_collator(features)\n",
    "\n",
    "\n",
    "samples = [lm_datasets['train'][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch['input_ids']:\n",
    "    print(f'\\n>>> {tokenizer.decode(chunk)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2ca5347-93b0-49d0-bfb5-9b4c6d3abc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 55182\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 6132\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size = len(lm_datasets['train'])\n",
    "train_size = int(data_size * 0.9)\n",
    "test_size = data_size - train_size\n",
    "\n",
    "downsampled_dataset = lm_datasets['train'].train_test_split(\n",
    "    train_size=train_size, test_size=test_size,\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a091b34-d654-45d3-ab65-93e04f6b1d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010848760604858398,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525ea554267b42e9a2b2ce617c923f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {'masked_' + k: v.numpy() for k, v in masked_inputs.items()}\n",
    "\n",
    "\n",
    "downsampled_dataset = downsampled_dataset.remove_columns(['word_ids'])\n",
    "eval_dataset = downsampled_dataset['test'].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=downsampled_dataset['test'].column_names,\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        'masked_input_ids': 'input_ids',\n",
    "        'masked_attention_mask': 'attention_mask',\n",
    "        'masked_labels': 'labels',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76b1324a-17d3-4937-9f9b-7db3aa9b8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    downsampled_dataset['train'],\n",
    "    shuffle=True,\n",
    "    batch_size=params.batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=params.batch_size, collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4bc3628-108b-4d92-a3af-01dd114c239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "optimizer = AdamW(model.parameters(), lr=params.learning_rate)\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55f1d6f8-ef6c-405f-8fcc-c3d9a1af7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = params.epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e60c57a-dc5e-4e5f-af31-01e15cc8e390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011129140853881836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10350,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500c642e34364e59a49242913aa78109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0: Perplexity: 10.342743873596191\n",
      ">>> Epoch 1: Perplexity: 9.660048484802246\n",
      ">>> Epoch 2: Perplexity: 9.189760208129883\n",
      ">>> Epoch 3: Perplexity: 8.874861717224121\n",
      ">>> Epoch 4: Perplexity: 8.65322494506836\n",
      ">>> Epoch 5: Perplexity: 8.56091022491455\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "output_dir = f'{params.model_name}-finetuned-imdb-accelerate'\n",
    "\n",
    "for epoch in range(params.epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(params.batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = torch.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61f55daa-e033-4146-8e29-8649be21c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "fill_masker = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a33985c-7951-4494-80af-ad6f7d052217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4944359362125397,\n",
       "  'token': 3185,\n",
       "  'token_str': 'movie',\n",
       "  'sequence': 'this is a great movie.'},\n",
       " {'score': 0.2802162766456604,\n",
       "  'token': 2143,\n",
       "  'token_str': 'film',\n",
       "  'sequence': 'this is a great film.'},\n",
       " {'score': 0.014623284339904785,\n",
       "  'token': 2265,\n",
       "  'token_str': 'show',\n",
       "  'sequence': 'this is a great show.'},\n",
       " {'score': 0.011867503635585308,\n",
       "  'token': 2028,\n",
       "  'token_str': 'one',\n",
       "  'sequence': 'this is a great one.'},\n",
       " {'score': 0.011748380027711391,\n",
       "  'token': 2466,\n",
       "  'token_str': 'story',\n",
       "  'sequence': 'this is a great story.'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_masker('This is a great [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da21d32-ae39-4bf0-bf17-7098d5b2d944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
