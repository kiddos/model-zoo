{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd48869c",
   "metadata": {},
   "source": [
    "## Pre-train BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "101e22d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc55c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # tokenizer hyperparameters\n",
    "    VOCAB_SIZE = 30000\n",
    "    LIMIT_ALPHABET = 1000\n",
    "    MIN_TOKEN_FREQ = 2\n",
    "    SPECIAL_TOKENS = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    "    WORDPIECES_PREFIX = '##'\n",
    "    MAX_LEN = 360\n",
    "    PROB_OUTPUT_MASK = 0.15\n",
    "    PROB_INPUT_MASK = 0.90\n",
    "    PROB_INPUT_RANDOM = 0.1\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    LR = 1e-3\n",
    "    EPSILON = 1e-6\n",
    "    BUFFER_SIZE = 10000\n",
    "    EPOCHS = 10\n",
    "    DROPOUT = 0.1\n",
    "    \n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8\n",
    "    FF_DIM = 128\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d55c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfds.load('imdb_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e3ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset['train']\n",
    "test_ds = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75a87ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=25000>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.cardinality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c219d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=25000>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds.cardinality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef25c524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=30000, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=True, lowercase=True, wordpieces_prefix=##)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text(train_ds, test_ds):\n",
    "    for entry in train_ds:\n",
    "        text = entry['text']\n",
    "        yield text.numpy().decode('utf-8')\n",
    "    for entry in test_ds:\n",
    "        text = entry['text']\n",
    "        yield text.numpy().decode('utf-8')\n",
    "\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=True,\n",
    "    lowercase=True,\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    get_text(train_ds, test_ds),\n",
    "    vocab_size=config.VOCAB_SIZE,\n",
    "    min_frequency=config.MIN_TOKEN_FREQ,\n",
    "    show_progress=True,\n",
    "    special_tokens=config.SPECIAL_TOKENS,\n",
    "    limit_alphabet=config.LIMIT_ALPHABET,\n",
    "    wordpieces_prefix=config.WORDPIECES_PREFIX,\n",
    ")\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddeb0854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pushing': 7798,\n",
       " 'deck': 11143,\n",
       " 'phool': 28649,\n",
       " '##eland': 7864,\n",
       " 'humanoids': 25770,\n",
       " 'defense': 7915,\n",
       " 'singers': 9865,\n",
       " 'rel': 773,\n",
       " '##qt': 24278,\n",
       " 'declared': 15420,\n",
       " 'catalog': 12129,\n",
       " 'heartless': 16064,\n",
       " 'esp': 7045,\n",
       " '##yn': 1526,\n",
       " 'chom': 19768,\n",
       " 'monster': 2201,\n",
       " 'medall': 28719,\n",
       " 'indiana': 11597,\n",
       " 'decapitated': 18760,\n",
       " 'abrasive': 25067,\n",
       " '4th': 10718,\n",
       " 'silliest': 22647,\n",
       " 'ron': 4235,\n",
       " 'vort': 22755,\n",
       " '##abilities': 13066,\n",
       " 'inquisition': 27756,\n",
       " 'speeches': 10605,\n",
       " 'baloo': 27523,\n",
       " 'zion': 29881,\n",
       " '##seud': 6504,\n",
       " 'vanilla': 15117,\n",
       " 'noooo': 28472,\n",
       " 'overlo': 21005,\n",
       " 'modest': 9358,\n",
       " 'macy': 7973,\n",
       " 'mahat': 24545,\n",
       " 'pisc': 29868,\n",
       " 'winter': 6938,\n",
       " 'aime': 13866,\n",
       " 'george': 2161,\n",
       " '##where': 1793,\n",
       " '##ervice': 17881,\n",
       " 'ellie': 28452,\n",
       " 'khan': 6440,\n",
       " 'reminisc': 5778,\n",
       " 'ronald': 10061,\n",
       " '##math': 11764,\n",
       " 'whereabouts': 17826,\n",
       " '##medi': 2781,\n",
       " 'trem': 12127,\n",
       " '##lit': 25141,\n",
       " 'mcgowan': 26479,\n",
       " 'scr': 3909,\n",
       " 'ligh': 11319,\n",
       " '##athom': 11854,\n",
       " 'mess': 1306,\n",
       " 'surgeon': 13162,\n",
       " 'melvyn': 16081,\n",
       " 'quix': 26784,\n",
       " 'willie': 9619,\n",
       " 'ineptly': 21325,\n",
       " '##ath': 611,\n",
       " '##word': 15324,\n",
       " '##ugoslav': 16405,\n",
       " 'evident': 7093,\n",
       " '##orn': 1622,\n",
       " 'distance': 7295,\n",
       " 'committing': 12910,\n",
       " 'phillipines': 27057,\n",
       " 'daniel': 4492,\n",
       " 'blood': 1370,\n",
       " 'reading': 2488,\n",
       " 'sidewalk': 12994,\n",
       " 'nek': 24728,\n",
       " 'resonance': 18126,\n",
       " 'teachings': 23648,\n",
       " 'argh': 27951,\n",
       " 'struggles': 6167,\n",
       " 'squeaky': 21942,\n",
       " 'jannings': 23694,\n",
       " 'replacing': 15015,\n",
       " 'rebu': 15414,\n",
       " '##erving': 19059,\n",
       " '##gore': 25647,\n",
       " 'references': 4247,\n",
       " 'exploration': 9163,\n",
       " 'categorize': 28369,\n",
       " 'protection': 12072,\n",
       " 'softer': 28364,\n",
       " 'pressburger': 28804,\n",
       " 'refer': 2408,\n",
       " 'section': 5281,\n",
       " 'wob': 16300,\n",
       " 'downhill': 8238,\n",
       " 'aweigh': 21055,\n",
       " 'citizens': 8319,\n",
       " 'greenstreet': 16849,\n",
       " 'villains': 4439,\n",
       " 'uneasy': 14014,\n",
       " 'chiba': 15083,\n",
       " 'dense': 16002,\n",
       " 'fisherman': 20196,\n",
       " 'bates': 7575,\n",
       " '##chen': 10169,\n",
       " 'inclined': 12958,\n",
       " 'cac': 20554,\n",
       " 'mockingbird': 25970,\n",
       " 'sanitized': 26388,\n",
       " 'restores': 26859,\n",
       " 'sooooo': 22670,\n",
       " 'pena': 25853,\n",
       " 'max': 3588,\n",
       " 'sara': 9044,\n",
       " '##mir': 28512,\n",
       " 'ozzy': 21921,\n",
       " 'woodstock': 23619,\n",
       " 'grossing': 24504,\n",
       " 'domination': 17340,\n",
       " 'jekyll': 20544,\n",
       " 'matte': 12504,\n",
       " 'again': 649,\n",
       " 'bawdy': 27143,\n",
       " '##ldon': 27308,\n",
       " 'mistakenly': 17544,\n",
       " 'plim': 29956,\n",
       " 'nicolas': 11089,\n",
       " 'pupils': 22683,\n",
       " '##eche': 20430,\n",
       " 'empowerment': 29049,\n",
       " 'sizzling': 28386,\n",
       " 'bear': 4210,\n",
       " '##lia': 17430,\n",
       " 'inherits': 21343,\n",
       " 'interpretation': 6013,\n",
       " 'civic': 28774,\n",
       " 'minus': 8539,\n",
       " 'minors': 27987,\n",
       " 'feminine': 12380,\n",
       " 'erroll': 27596,\n",
       " 'hayden': 16859,\n",
       " '##wer': 9479,\n",
       " '##ft': 777,\n",
       " 'narrate': 26918,\n",
       " '##ista': 14605,\n",
       " 'historic': 10012,\n",
       " '##orloos': 29167,\n",
       " 'rare': 3334,\n",
       " '##ass': 515,\n",
       " 'syber': 26862,\n",
       " 'rousing': 12968,\n",
       " 'tsun': 23023,\n",
       " 'lands': 8485,\n",
       " 'cough': 12392,\n",
       " '##otomy': 19319,\n",
       " 'busty': 25441,\n",
       " 'errol': 10321,\n",
       " 'without': 885,\n",
       " '##alow': 27286,\n",
       " 'chariot': 29974,\n",
       " 'marked': 9811,\n",
       " 'yard': 8756,\n",
       " 'differences': 7586,\n",
       " 'venge': 6870,\n",
       " '##pool': 15876,\n",
       " 'skin': 3915,\n",
       " 'armor': 14769,\n",
       " 'privilege': 15429,\n",
       " 'distributors': 17563,\n",
       " 'pattern': 8053,\n",
       " 'restoring': 26860,\n",
       " '##iness': 1432,\n",
       " 'blo': 14074,\n",
       " 'gist': 22387,\n",
       " 'created': 2784,\n",
       " 'dess': 16164,\n",
       " 'pathological': 28748,\n",
       " 'bravo': 9268,\n",
       " '##ments': 1166,\n",
       " 'fountain': 20272,\n",
       " 'beller': 25868,\n",
       " '##ril': 13875,\n",
       " 'recommended': 2964,\n",
       " 'oozes': 20520,\n",
       " 'invaded': 21816,\n",
       " 'mature': 5529,\n",
       " 'propel': 22880,\n",
       " '¿': 92,\n",
       " 'compensate': 13203,\n",
       " 'manhood': 21213,\n",
       " 'afghan': 9270,\n",
       " 'feminist': 9980,\n",
       " '##stay': 26722,\n",
       " 'dunno': 16422,\n",
       " 'muss': 19111,\n",
       " 'nitpick': 23807,\n",
       " '1968': 8588,\n",
       " '1975': 11584,\n",
       " 'strained': 13718,\n",
       " 'toilets': 22639,\n",
       " 'mano': 29227,\n",
       " 'aut': 3448,\n",
       " 'reeve': 14429,\n",
       " 'imagination': 3799,\n",
       " 'spectre': 23309,\n",
       " 'hod': 25109,\n",
       " 'aton': 27313,\n",
       " 'sophia': 16391,\n",
       " 'slimy': 10778,\n",
       " '##tail': 15073,\n",
       " '##gen': 3294,\n",
       " 'guevara': 18358,\n",
       " 'neander': 26752,\n",
       " 'smashes': 23230,\n",
       " 'abject': 27325,\n",
       " 'cray': 20908,\n",
       " 'tennessee': 20545,\n",
       " 'applicable': 29691,\n",
       " 'persistent': 19625,\n",
       " '1936': 11408,\n",
       " 'wife': 1211,\n",
       " 'occult': 12835,\n",
       " 'exting': 24720,\n",
       " 'conducts': 26493,\n",
       " 'campy': 5606,\n",
       " 'connections': 10242,\n",
       " 'kidnapping': 10512,\n",
       " 'conj': 11259,\n",
       " 'unmistakably': 27128,\n",
       " 'deepest': 15113,\n",
       " 'acknowled': 11613,\n",
       " 'www': 9735,\n",
       " 'rufus': 28106,\n",
       " '##ensions': 10100,\n",
       " 'industries': 28151,\n",
       " 'gideon': 24235,\n",
       " 'reward': 5882,\n",
       " 'ongoing': 14834,\n",
       " 'album': 8195,\n",
       " 'awfully': 9160,\n",
       " 'remastered': 24773,\n",
       " 'peace': 4286,\n",
       " '19': 620,\n",
       " 'transfer': 6361,\n",
       " 'wond': 817,\n",
       " 'generates': 19379,\n",
       " 'displayed': 8295,\n",
       " 'collins': 11656,\n",
       " 'shah': 8685,\n",
       " 'wall': 2362,\n",
       " 'magician': 14197,\n",
       " '##verted': 8823,\n",
       " 'zoo': 16303,\n",
       " 'cutting': 5043,\n",
       " 'poisonous': 22281,\n",
       " 'cillian': 18167,\n",
       " 'transp': 14277,\n",
       " 'comprehend': 11619,\n",
       " 'chay': 20039,\n",
       " 'foxes': 22588,\n",
       " 'reloaded': 23798,\n",
       " 'trainer': 19401,\n",
       " 'stepford': 22909,\n",
       " 'negatively': 24486,\n",
       " '##bro': 15879,\n",
       " 'transparent': 14705,\n",
       " '##ogram': 15626,\n",
       " '##card': 21451,\n",
       " 'brook': 4154,\n",
       " 'observation': 11549,\n",
       " 'realms': 24762,\n",
       " 'undoing': 28760,\n",
       " 'bounces': 22264,\n",
       " 'nobody': 3122,\n",
       " 'informant': 28145,\n",
       " 'umpt': 23728,\n",
       " 'tus': 26683,\n",
       " 'cheese': 5309,\n",
       " 'campaign': 8559,\n",
       " 'cohesive': 14450,\n",
       " 'perfection': 6082,\n",
       " 'breillat': 24044,\n",
       " 'quit': 9053,\n",
       " '##sy': 1219,\n",
       " 'lie': 4194,\n",
       " 'gorilla': 11178,\n",
       " 'asserts': 25493,\n",
       " 'schooler': 25312,\n",
       " 'counterparts': 14042,\n",
       " 'cans': 19343,\n",
       " 'mu': 452,\n",
       " 'reven': 24299,\n",
       " '##ultimate': 24364,\n",
       " '##ic': 210,\n",
       " 'kurt': 7593,\n",
       " '##ght': 293,\n",
       " 'legally': 18484,\n",
       " 'disguises': 18737,\n",
       " 'glorify': 20734,\n",
       " 'tweed': 24153,\n",
       " 'trois': 26187,\n",
       " 'vividly': 12622,\n",
       " 'amel': 16331,\n",
       " 'kent': 8678,\n",
       " 'pristine': 22455,\n",
       " '[UNK]': 1,\n",
       " 'exchang': 13845,\n",
       " 'spotlight': 12686,\n",
       " 'with': 251,\n",
       " 'epatha': 29325,\n",
       " 'neighbourhood': 28310,\n",
       " 'bou': 13040,\n",
       " 'lethargic': 19982,\n",
       " 'downtrodden': 25591,\n",
       " 'deniz': 25358,\n",
       " '##rates': 11044,\n",
       " 'scope': 8634,\n",
       " 'catching': 8617,\n",
       " 'graduates': 21937,\n",
       " 'breckin': 22020,\n",
       " '##cking': 18010,\n",
       " 'institutional': 24961,\n",
       " 'bemo': 27288,\n",
       " 'naturalism': 24469,\n",
       " 'reviewers': 4655,\n",
       " '##eve': 8173,\n",
       " 'headquar': 21524,\n",
       " 'neat': 5289,\n",
       " 'momentum': 12756,\n",
       " 'nud': 2664,\n",
       " '##case': 10029,\n",
       " 'dealt': 6684,\n",
       " 'collectively': 26937,\n",
       " 'roscoe': 20169,\n",
       " 'assed': 22160,\n",
       " 'garry': 22188,\n",
       " 'correl': 27435,\n",
       " 'mod': 1337,\n",
       " 'explaining': 7713,\n",
       " 'inexper': 11948,\n",
       " 'fet': 4548,\n",
       " '##chy': 5935,\n",
       " 'jeopardy': 18580,\n",
       " 'temperament': 28275,\n",
       " 'popping': 10448,\n",
       " 'panama': 28162,\n",
       " 'bat': 1373,\n",
       " 'wolfe': 21076,\n",
       " 'nodding': 23371,\n",
       " 'nearby': 7350,\n",
       " 'vene': 11095,\n",
       " 'traveler': 23662,\n",
       " 'bogos': 17201,\n",
       " 'regrettably': 23339,\n",
       " 'johans': 16731,\n",
       " 'leaf': 18257,\n",
       " 'antichrist': 22552,\n",
       " '##zoff': 28492,\n",
       " 'labyrinth': 15725,\n",
       " 'choreograph': 8554,\n",
       " 'spoofed': 27550,\n",
       " 'noodle': 24256,\n",
       " 'explos': 4034,\n",
       " '190': 13595,\n",
       " 'perpetrators': 26463,\n",
       " 'undermines': 21379,\n",
       " 'gass': 17600,\n",
       " 'boob': 12009,\n",
       " 'archetypal': 20798,\n",
       " 'diaries': 21480,\n",
       " '##odies': 3971,\n",
       " 'obstacles': 13644,\n",
       " 'hunters': 9171,\n",
       " 'pictures': 3154,\n",
       " 'riot': 8844,\n",
       " 'acerb': 29330,\n",
       " '##ead': 903,\n",
       " 'bew': 6077,\n",
       " 'program': 3259,\n",
       " 'dealing': 4407,\n",
       " 'beowulf': 14230,\n",
       " 'dart': 26655,\n",
       " '##store': 18627,\n",
       " 'sorrows': 27190,\n",
       " 'dirk': 14294,\n",
       " 'kettle': 27120,\n",
       " '”': 112,\n",
       " 'songwriter': 21843,\n",
       " 'spliced': 16283,\n",
       " 'simplicity': 9369,\n",
       " 'terminal': 15965,\n",
       " 'separation': 16389,\n",
       " 'magnetic': 19877,\n",
       " 'corb': 8935,\n",
       " 'luther': 15314,\n",
       " 'lionel': 10626,\n",
       " 'heaving': 28547,\n",
       " '##encia': 29916,\n",
       " '105': 27393,\n",
       " '##ished': 2135,\n",
       " '##entino': 24705,\n",
       " '22': 8560,\n",
       " '##atham': 16495,\n",
       " 'gogg': 23929,\n",
       " 'pancakes': 28419,\n",
       " 'omnipresent': 28412,\n",
       " 'bel': 642,\n",
       " 'lic': 8781,\n",
       " '##wn': 608,\n",
       " 'grabbing': 16583,\n",
       " 'mercenary': 18350,\n",
       " 'critique': 11711,\n",
       " 'colombian': 24188,\n",
       " 'wasn': 1132,\n",
       " 'schedule': 13204,\n",
       " 'paycheck': 14132,\n",
       " '##alid': 16469,\n",
       " 'dus': 17093,\n",
       " 'narrating': 19854,\n",
       " 'shipped': 25669,\n",
       " '##icking': 8925,\n",
       " 'hisa': 28553,\n",
       " 'denise': 15661,\n",
       " '##hou': 16307,\n",
       " 'drisc': 29265,\n",
       " 'collecting': 14889,\n",
       " 'kingsley': 13387,\n",
       " 'craw': 5135,\n",
       " 'organizations': 22611,\n",
       " 'buster': 7636,\n",
       " 'posse': 15001,\n",
       " 'nol': 6188,\n",
       " '##wyn': 17426,\n",
       " 'bitten': 10580,\n",
       " '##areness': 12037,\n",
       " 'needles': 20645,\n",
       " '##pectives': 13794,\n",
       " 'tearjerker': 22362,\n",
       " 'deodato': 24223,\n",
       " 'segal': 11480,\n",
       " 'funn': 2764,\n",
       " 'priv': 3560,\n",
       " '##atched': 8301,\n",
       " '##aches': 14489,\n",
       " 'nope': 11106,\n",
       " 'tc': 10587,\n",
       " 'fant': 1474,\n",
       " 'learnt': 18922,\n",
       " 'torch': 13367,\n",
       " 'jefferson': 14663,\n",
       " 'instincts': 13470,\n",
       " 'bam': 7164,\n",
       " 'one': 295,\n",
       " 'well': 435,\n",
       " 'familial': 25881,\n",
       " 'elfman': 27723,\n",
       " 'delight': 2855,\n",
       " 'irresistible': 15174,\n",
       " 'whines': 24294,\n",
       " 'c': 45,\n",
       " 'throw': 3070,\n",
       " '94': 25098,\n",
       " 'pd': 28479,\n",
       " 'disagree': 5950,\n",
       " '##ilo': 18625,\n",
       " 'releasing': 12602,\n",
       " 'shape': 5952,\n",
       " 'oph': 22746,\n",
       " 'crooks': 13119,\n",
       " 'marque': 20923,\n",
       " 'jal': 29106,\n",
       " 'incidental': 13932,\n",
       " '##umph': 6133,\n",
       " 'socialist': 18076,\n",
       " 'ackroyd': 24526,\n",
       " 'troopers': 13420,\n",
       " 'bizarre': 2899,\n",
       " 'fid': 7209,\n",
       " 'bodyguard': 14048,\n",
       " 'adrienne': 19955,\n",
       " 'danning': 18905,\n",
       " '##sexual': 17627,\n",
       " 'consummate': 18732,\n",
       " 'rife': 22060,\n",
       " 'ds9': 27144,\n",
       " 'alternative': 7741,\n",
       " '##alez': 26843,\n",
       " 'bailey': 17533,\n",
       " 'herring': 17634,\n",
       " 'sit': 1146,\n",
       " 'workaholic': 26637,\n",
       " 'tel': 13320,\n",
       " 'longs': 20353,\n",
       " 'misogyn': 14451,\n",
       " 'ami': 29984,\n",
       " 'announced': 13636,\n",
       " 'merciless': 12515,\n",
       " 'scrubs': 22962,\n",
       " 'rodri': 12615,\n",
       " 'review': 1182,\n",
       " 'relatives': 8833,\n",
       " 'earth': 1932,\n",
       " 'doyle': 12180,\n",
       " 'sleaz': 21863,\n",
       " 'ogling': 28477,\n",
       " 'draws': 7524,\n",
       " 'waning': 26687,\n",
       " 'psyched': 11117,\n",
       " 'authoritarian': 29469,\n",
       " 'change': 2021,\n",
       " 'learned': 4272,\n",
       " 'pes': 11432,\n",
       " 'repressed': 11743,\n",
       " 'claw': 10988,\n",
       " '##won': 24270,\n",
       " '##aders': 12570,\n",
       " 'approve': 18547,\n",
       " 'altered': 10465,\n",
       " 'unpre': 15560,\n",
       " 'bodily': 19949,\n",
       " 'located': 9912,\n",
       " 'scorn': 17276,\n",
       " 'gemser': 25478,\n",
       " 'perils': 25230,\n",
       " 'conch': 26174,\n",
       " 'monoch': 26248,\n",
       " 'greasy': 21539,\n",
       " 'cockpit': 21926,\n",
       " 'stomachs': 29546,\n",
       " 'original': 857,\n",
       " 'documentaries': 7100,\n",
       " 'pitched': 13194,\n",
       " 'cryst': 7138,\n",
       " 'brash': 12039,\n",
       " '##blahblahblahblah': 19957,\n",
       " 'pyramid': 22717,\n",
       " 'uncompr': 14780,\n",
       " 'corps': 19822,\n",
       " 'leon': 4639,\n",
       " 'veteran': 5174,\n",
       " 'tanner': 21357,\n",
       " 'members': 2736,\n",
       " 'encomp': 16370,\n",
       " 'ineffectual': 22323,\n",
       " 'disheveled': 29041,\n",
       " 'suzanne': 16289,\n",
       " 'rochon': 22361,\n",
       " 'names': 3322,\n",
       " 'architecture': 15157,\n",
       " 'zucco': 16272,\n",
       " 'grenade': 20515,\n",
       " 'rudolph': 18742,\n",
       " 'gator': 22388,\n",
       " '-': 17,\n",
       " 'crying': 5321,\n",
       " 'sandy': 10143,\n",
       " 'pokemon': 11033,\n",
       " 'disbelief': 5945,\n",
       " 'backwood': 13594,\n",
       " '##ioli': 26693,\n",
       " 'teases': 26779,\n",
       " 'longed': 28644,\n",
       " '##onse': 6772,\n",
       " '##ika': 7956,\n",
       " '##biz': 29897,\n",
       " 'forgetting': 12839,\n",
       " 'riveted': 21621,\n",
       " 'voyeurism': 24197,\n",
       " 'shaggy': 14564,\n",
       " '##hler': 26702,\n",
       " '##athan': 4721,\n",
       " 'jas': 27822,\n",
       " 'deliverance': 12840,\n",
       " 'horrified': 11265,\n",
       " 'shouted': 23367,\n",
       " 'leno': 12017,\n",
       " 'maver': 15618,\n",
       " 'accentuated': 28143,\n",
       " 'arabian': 24534,\n",
       " '##iring': 3369,\n",
       " 'guarante': 9665,\n",
       " '##pher': 3521,\n",
       " 'psychology': 10003,\n",
       " 'notebook': 18068,\n",
       " 'marquis': 16814,\n",
       " 'sidekicks': 20743,\n",
       " 'fuzzy': 12354,\n",
       " 'hannibal': 15980,\n",
       " '##dome': 26699,\n",
       " '##ann': 1302,\n",
       " 'ed': 794,\n",
       " 'oblivion': 15288,\n",
       " 'compass': 24790,\n",
       " 'ladies': 4659,\n",
       " 'imitation': 9190,\n",
       " '##icus': 19524,\n",
       " '54': 18365,\n",
       " '##illas': 21469,\n",
       " 'spewing': 19106,\n",
       " '##ola': 5195,\n",
       " 'nanny': 12559,\n",
       " 'fathom': 15310,\n",
       " 'programme': 10241,\n",
       " 'startlingly': 27653,\n",
       " 'aesth': 8625,\n",
       " '##urer': 16319,\n",
       " '##hoff': 14352,\n",
       " 'choose': 4773,\n",
       " 'daphne': 16160,\n",
       " '##iving': 4600,\n",
       " 'disappointments': 17493,\n",
       " 'reservations': 18564,\n",
       " 'sassy': 13050,\n",
       " 'esch': 22504,\n",
       " 'swat': 18050,\n",
       " 'diagnosis': 25038,\n",
       " 'gorehounds': 25570,\n",
       " 'potbo': 18925,\n",
       " 'medical': 5906,\n",
       " 'gree': 5147,\n",
       " 'rewarded': 12144,\n",
       " 'bic': 13041,\n",
       " 'benedict': 19607,\n",
       " 'pupil': 23418,\n",
       " 'incidentally': 9926,\n",
       " 'peripher': 19090,\n",
       " 'ø': 96,\n",
       " 'wildly': 9060,\n",
       " 'pleasures': 12997,\n",
       " '##ho': 10422,\n",
       " 'cusack': 7453,\n",
       " '##andering': 26766,\n",
       " 'gibberish': 19717,\n",
       " 'intentioned': 18291,\n",
       " 'sleuth': 18467,\n",
       " 'enthralled': 16138,\n",
       " '##lix': 10852,\n",
       " 'exclaim': 18907,\n",
       " '##p': 132,\n",
       " 'imminent': 22574,\n",
       " 'computer': 3219,\n",
       " 'profess': 2334,\n",
       " '##tered': 8206,\n",
       " '##med': 1464,\n",
       " 'frenchman': 17693,\n",
       " 'corporations': 18733,\n",
       " 'jayne': 22258,\n",
       " '##arch': 1935,\n",
       " 'prevents': 16253,\n",
       " 'offed': 24761,\n",
       " 'morning': 4279,\n",
       " 'ned': 7338,\n",
       " 'instructions': 20498,\n",
       " 'parasite': 25531,\n",
       " '##oil': 22766,\n",
       " 'milligan': 16079,\n",
       " '##asion': 7554,\n",
       " 'pang': 17257,\n",
       " 'rt': 28482,\n",
       " 'syru': 21836,\n",
       " 'kovacs': 29048,\n",
       " 'levity': 29224,\n",
       " 'twisting': 17186,\n",
       " 'watch': 389,\n",
       " 'glory': 6511,\n",
       " 'gram': 10179,\n",
       " 'interpol': 28799,\n",
       " 'barrie': 27615,\n",
       " 'uncovers': 24004,\n",
       " '##uva': 25636,\n",
       " 'camper': 25277,\n",
       " 'femin': 5941,\n",
       " 'currently': 7330,\n",
       " '##se': 237,\n",
       " 'terrorist': 7224,\n",
       " '##wright': 10028,\n",
       " 'reagan': 11196,\n",
       " 'intents': 23202,\n",
       " 'steadfast': 27613,\n",
       " 'neil': 6916,\n",
       " 'wann': 6550,\n",
       " 'overacts': 14426,\n",
       " 'okay': 2304,\n",
       " 'mc': 2127,\n",
       " 'content': 3553,\n",
       " 'wealth': 4618,\n",
       " 'rhymes': 19947,\n",
       " 'krabbe': 21078,\n",
       " '##appe': 3179,\n",
       " 'scumb': 22090,\n",
       " 'archaic': 25383,\n",
       " 'joker': 13694,\n",
       " 'blown': 5458,\n",
       " 'hacker': 20435,\n",
       " 'gran': 25251,\n",
       " 'dissatisfied': 25554,\n",
       " 'traditional': 4771,\n",
       " 'arcs': 19449,\n",
       " 'attentions': 22705,\n",
       " 'scound': 25203,\n",
       " 'narrow': 9463,\n",
       " '##rence': 5524,\n",
       " 'murdering': 9493,\n",
       " '##parents': 15400,\n",
       " 'peta': 29372,\n",
       " 'rar': 14580,\n",
       " 'marathon': 13516,\n",
       " 'electricity': 13638,\n",
       " 'reunited': 15412,\n",
       " 'aschenbach': 18402,\n",
       " 'ian': 7276,\n",
       " 'laps': 15867,\n",
       " 'screenings': 21239,\n",
       " '##yard': 10168,\n",
       " 'sustain': 11680,\n",
       " 'expresses': 14885,\n",
       " 'py': 9713,\n",
       " '##feratu': 28131,\n",
       " 'harsh': 4918,\n",
       " 'youthful': 11483,\n",
       " 'aquarium': 25559,\n",
       " 'agony': 12045,\n",
       " 'clunk': 26191,\n",
       " 'kem': 27824,\n",
       " 'midget': 12752,\n",
       " 'solution': 8365,\n",
       " 'portrays': 4947,\n",
       " 'indelible': 22714,\n",
       " 'alexand': 9098,\n",
       " 'stomping': 21999,\n",
       " 'smattering': 25735,\n",
       " 'returns': 3867,\n",
       " 'hurts': 9030,\n",
       " '##mila': 20858,\n",
       " 'unmemorable': 22727,\n",
       " 'tint': 11627,\n",
       " 'underwritten': 21230,\n",
       " 'abounds': 24997,\n",
       " 'sadly': 2834,\n",
       " 'mushrooms': 27116,\n",
       " 'has': 361,\n",
       " 'fundamental': 9743,\n",
       " '##sies': 20012,\n",
       " '##enas': 22431,\n",
       " 'pondering': 20467,\n",
       " 'pays': 7858,\n",
       " 'looney': 12920,\n",
       " 'accentuate': 28770,\n",
       " 'suspension': 10051,\n",
       " '##othing': 7146,\n",
       " 'cum': 8752,\n",
       " 'ring': 3355,\n",
       " '##ators': 4067,\n",
       " 'inad': 11767,\n",
       " 'appreciates': 21274,\n",
       " 'react': 2760,\n",
       " 'walter': 5314,\n",
       " 'senile': 18968,\n",
       " 'projector': 22208,\n",
       " 'unraveling': 25949,\n",
       " '##och': 6074,\n",
       " 'bottoms': 20992,\n",
       " 'django': 27070,\n",
       " 'construction': 8998,\n",
       " 'sundance': 12940,\n",
       " 'mining': 18026,\n",
       " 'shimizu': 28974,\n",
       " 'unaw': 9224,\n",
       " 'tarr': 25623,\n",
       " 'basil': 12745,\n",
       " 'austin': 8671,\n",
       " '##iven': 4422,\n",
       " 'haig': 29193,\n",
       " 'chin': 10485,\n",
       " 'miami': 12136,\n",
       " 'likes': 3035,\n",
       " 'assets': 18684,\n",
       " 'wright': 10477,\n",
       " '1991': 11009,\n",
       " 'conduct': 8703,\n",
       " 'gazing': 24586,\n",
       " '##uggish': 16084,\n",
       " 'shrill': 16562,\n",
       " 'monotonous': 13481,\n",
       " 'records': 10144,\n",
       " 'showcase': 8623,\n",
       " 'ancest': 12711,\n",
       " '##through': 15057,\n",
       " '##anaz': 24288,\n",
       " 'lesley': 22575,\n",
       " 'steiner': 18901,\n",
       " 'lommel': 20839,\n",
       " 'where': 583,\n",
       " '##zar': 24271,\n",
       " 'clubs': 13098,\n",
       " '##ician': 23892,\n",
       " '##gether': 1142,\n",
       " 'flawless': 7200,\n",
       " 'ruggles': 27195,\n",
       " '##nals': 29155,\n",
       " 'together': 1143,\n",
       " 'static': 9386,\n",
       " 'binoche': 16158,\n",
       " 'graceful': 18280,\n",
       " 'woodward': 21857,\n",
       " 'shudder': 16753,\n",
       " 'gradu': 6421,\n",
       " 'libr': 5883,\n",
       " 'injured': 9708,\n",
       " '##plo': 8964,\n",
       " 'cheyenne': 20816,\n",
       " 'impacted': 26386,\n",
       " '##vous': 26706,\n",
       " 'defined': 8444,\n",
       " '##escope': 27273,\n",
       " 'weller': 21782,\n",
       " 'impression': 3200,\n",
       " 'uphe': 23925,\n",
       " 'hassle': 27081,\n",
       " 'legs': 6366,\n",
       " 'mank': 27339,\n",
       " '##zac': 18609,\n",
       " 'copy': 2498,\n",
       " 'hugh': 5811,\n",
       " 'nearly': 2314,\n",
       " 'hostess': 21894,\n",
       " 'unintelligible': 18574,\n",
       " 'hudson': 6711,\n",
       " '##gir': 8821,\n",
       " 'awes': 27375,\n",
       " '##gens': 29899,\n",
       " 'response': 6910,\n",
       " 'outline': 11260,\n",
       " 'doo': 7009,\n",
       " 'stiller': 9426,\n",
       " 'cash': 4369,\n",
       " 'amok': 15222,\n",
       " 'dispose': 21915,\n",
       " 'joa': 23957,\n",
       " 'cambod': 14629,\n",
       " 'aard': 22036,\n",
       " 'sits': 8322,\n",
       " '##arians': 19313,\n",
       " '##bows': 24683,\n",
       " 'tobias': 21704,\n",
       " 'notable': 6025,\n",
       " 'reperto': 17066,\n",
       " '1934': 12007,\n",
       " 'imbecilic': 29026,\n",
       " 'feeling': 1794,\n",
       " 'recon': 8066,\n",
       " 'unjust': 13332,\n",
       " 'cliches': 3644,\n",
       " 'subtle': 2730,\n",
       " 'ensured': 28809,\n",
       " 'ou': 6063,\n",
       " '##orters': 10299,\n",
       " 'misery': 8791,\n",
       " 'finn': 11268,\n",
       " 'inki': 29172,\n",
       " 'hedge': 19733,\n",
       " 'quadriple': 28989,\n",
       " 'whims': 13550,\n",
       " 'pag': 16610,\n",
       " 'hoff': 28646,\n",
       " '##des': 16306,\n",
       " 'incredibly': 2595,\n",
       " 'greet': 14172,\n",
       " 'czechoslovak': 29086,\n",
       " 'humanity': 4293,\n",
       " '##ages': 1680,\n",
       " 'curator': 28071,\n",
       " 'bright': 3774,\n",
       " 'videos': 6324,\n",
       " 'issue': 4158,\n",
       " 'citizen': 6959,\n",
       " 'por': 10980,\n",
       " 'hk': 10784,\n",
       " 'libid': 21202,\n",
       " '##ya': 5191,\n",
       " 'whew': 26135,\n",
       " 'connolly': 20428,\n",
       " 'unac': 19088,\n",
       " '##made': 19057,\n",
       " 'incoher': 23366,\n",
       " 'jonny': 20704,\n",
       " 'readings': 23245,\n",
       " 'rampant': 15042,\n",
       " 'churches': 23677,\n",
       " '##itor': 3870,\n",
       " 'anthropologist': 25535,\n",
       " '##people': 14843,\n",
       " 'wrinkle': 24763,\n",
       " 'mini': 4438,\n",
       " 'grossly': 15956,\n",
       " 'masters': 6937,\n",
       " 'superf': 5503,\n",
       " 'starewicz': 21363,\n",
       " '##ulas': 24315,\n",
       " 'estr': 11122,\n",
       " '##blade': 26959,\n",
       " 'crab': 24799,\n",
       " 'overview': 26199,\n",
       " 'apes': 8476,\n",
       " 'popul': 5032,\n",
       " 'duk': 18266,\n",
       " 'kellerman': 22690,\n",
       " 'poisoning': 23726,\n",
       " 'dude': 5268,\n",
       " 'consum': 5637,\n",
       " '##ropriately': 8453,\n",
       " 'regarding': 5590,\n",
       " 'lump': 12557,\n",
       " '##say': 13052,\n",
       " 'minute': 2270,\n",
       " 'headlights': 23985,\n",
       " 'insurance': 11270,\n",
       " 'ker': 6611,\n",
       " 'beet': 13058,\n",
       " 'pey': 27351,\n",
       " 'ori': 780,\n",
       " 'panties': 16879,\n",
       " 'fanny': 13431,\n",
       " '##ium': 4261,\n",
       " 'hospitals': 21576,\n",
       " 'goddam': 23764,\n",
       " 'dribble': 21535,\n",
       " 'grand': 2123,\n",
       " 'bets': 19097,\n",
       " 'sapp': 24760,\n",
       " 'virtue': 14413,\n",
       " 'hud': 6215,\n",
       " 'mildly': 5545,\n",
       " '80s': 4029,\n",
       " 'gibson': 10292,\n",
       " '##isf': 2614,\n",
       " 'mundane': 9107,\n",
       " 'forbes': 17122,\n",
       " 'pastiche': 18461,\n",
       " 'clearer': 17695,\n",
       " 'flatliners': 18708,\n",
       " 'bleakness': 28858,\n",
       " '##ipp': 1774,\n",
       " '##dre': 18827,\n",
       " 'powerfully': 15787,\n",
       " '##cuts': 18392,\n",
       " 'substantially': 27072,\n",
       " '¾': 91,\n",
       " 'ah': 5390,\n",
       " 'cleveland': 19989,\n",
       " 'gor': 6334,\n",
       " 'creaky': 20359,\n",
       " 'philosophies': 26990,\n",
       " 'scanners': 27318,\n",
       " 'roland': 16598,\n",
       " 'machismo': 26934,\n",
       " 'attendant': 15279,\n",
       " '##heartedly': 20487,\n",
       " 'ivanna': 29532,\n",
       " '1972': 8266,\n",
       " 'ruled': 14537,\n",
       " 'evenly': 28585,\n",
       " 'opium': 28660,\n",
       " 'inh': 4692,\n",
       " 'brothel': 20805,\n",
       " 'screenplay': 2528,\n",
       " 'peers': 13158,\n",
       " '##tttt': 26442,\n",
       " 'easiest': 17494,\n",
       " 'hysterically': 12540,\n",
       " '##iffer': 6113,\n",
       " '##rec': 19316,\n",
       " 'jeon': 26883,\n",
       " 'drawback': 13616,\n",
       " 'application': 24196,\n",
       " 'quie': 26783,\n",
       " '##neg': 16806,\n",
       " '70s': 4217,\n",
       " 'bres': 25166,\n",
       " 'nath': 29119,\n",
       " 'exchange': 8346,\n",
       " 'heading': 9087,\n",
       " 'stephens': 22687,\n",
       " '##brush': 22915,\n",
       " 'private': 4472,\n",
       " 'indifference': 16260,\n",
       " 'awaited': 19666,\n",
       " 'rendezvous': 29795,\n",
       " 'ladd': 18084,\n",
       " 'creators': 6907,\n",
       " 'entertainingly': 28667,\n",
       " 'scratched': 24946,\n",
       " 'base': 3587,\n",
       " 'beards': 27883,\n",
       " '##riding': 27538,\n",
       " '##gers': 8252,\n",
       " 'brutality': 9821,\n",
       " 'vincente': 22631,\n",
       " 'cly': 14337,\n",
       " 'alt': 4072,\n",
       " 'waited': 8796,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deba2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.enable_truncation(max_length=config.MAX_LEN)\n",
    "tokenizer.enable_padding(length=config.MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7d8ee91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 360), dtype=int32, numpy=\n",
      "array([[   51,   240,  3497, ...,     0,     0,     0],\n",
      "       [13624,  1418,     4, ...,   331,   183,     4],\n",
      "       [ 1903,     4,   183, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [ 2625,    12,  9005, ...,     0,     0,     0],\n",
      "       [    4,   488,  1718, ...,     0,     0,     0],\n",
      "       [  241,    51,  1134, ...,     0,     0,     0]], dtype=int32)>, <tf.Tensor: shape=(32, 360), dtype=int32, numpy=\n",
      "array([[   51,   240,  3497, ...,     0,     0,     0],\n",
      "       [13624,  1418,   330, ...,   331,   183,   761],\n",
      "       [ 1903,   204,   183, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [ 2625,    12,  9005, ...,     0,     0,     0],\n",
      "       [  183,   488,  1718, ...,     0,     0,     0],\n",
      "       [  241,    51,  1134, ...,     0,     0,     0]], dtype=int32)>, <tf.Tensor: shape=(32, 360), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 1., ..., 0., 0., 1.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>)\n",
      "i was electrified [MASK] i first [MASK] this in 1983 or 1984. steven biko is gone half [MASK] through the film but the resonance of his courage [MASK] [MASK] is not forgotten. i didn'[MASK] when finally in south africa in 1993. it is also largely a story about friendship [MASK] loyalty. when i [MASK] in south africa and heard the audience at a dance recital in natal sing nkosi sikelel'i [MASK]rika, my hair stood on its ends. there is a [MASK] to learn from this story for all peoples. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "i was electrified when i first saw this in 1983 or 1984. steven biko is gone half way through the film but the resonance of his courage and wisdom is not forgotten. i didn't when finally in south africa in 1993. it is also largely a story about friendship and loyalty. when i was in south africa and heard the audience at a dance recital in natal sing nkosi sikelel'iafrika, my hair stood on its ends. there is a lot to learn from this story for all peoples. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "MASK_TOKEN_ID = tokenizer.token_to_id('[MASK]')\n",
    "\n",
    "\n",
    "def tokenize_tensor(tensor):\n",
    "    text = tensor.numpy().decode('utf-8')\n",
    "    result = tokenizer.encode(text, add_special_tokens=True)\n",
    "    ids = np.array(result.ids)\n",
    "    sentence_len = len(ids)\n",
    "    inp_mask = np.random.rand(sentence_len) < config.PROB_OUTPUT_MASK\n",
    "    # Do not mask special tokens\n",
    "    inp_mask[ids <= 2] = False\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    labels = -1 * np.ones([sentence_len], dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask] = ids[inp_mask]\n",
    "    # Prepare input\n",
    "    encoded_texts_masked = np.copy(ids)\n",
    "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
    "    # This means leaving 10% unchange\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(sentence_len) < config.PROB_INPUT_MASK)\n",
    "    encoded_texts_masked[inp_mask_2mask] = MASK_TOKEN_ID\n",
    "\n",
    "    # Set 10% to a random token\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(sentence_len) < config.PROB_INPUT_RANDOM)\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
    "        3, MASK_TOKEN_ID, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # Prepare sample_weights to pass to .fit() method\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # y_labels would be same as encoded_texts i.e input tokens\n",
    "    y_labels = np.copy(ids)\n",
    "    \n",
    "    encoded_texts_masked_tensor = tf.constant(encoded_texts_masked, tf.int32)\n",
    "    y_labels_tensor = tf.constant(y_labels, tf.int32)\n",
    "    sample_weights_tensor = tf.constant(sample_weights, tf.float32)\n",
    "    return encoded_texts_masked_tensor, y_labels_tensor, sample_weights_tensor\n",
    "\n",
    "\n",
    "def preprocess_text(entry):\n",
    "    example = tf.py_function(\n",
    "        tokenize_tensor, [entry['text']], Tout=(tf.int32, tf.int32, tf.float32))\n",
    "    encoded_texts_masked, y_labels, sample_weights = example\n",
    "    return encoded_texts_masked, y_labels, sample_weights\n",
    "\n",
    "\n",
    "def show_example(ds):\n",
    "    for entry in ds:\n",
    "        print(entry)\n",
    "        encoded_texts_masked, y_labels, sample_weights = entry\n",
    "        decoded_input = tokenizer.decode(encoded_texts_masked[0], skip_special_tokens=False)\n",
    "        decoded_output = tokenizer.decode(y_labels[0], skip_special_tokens=False)\n",
    "        print(decoded_input)\n",
    "        print(decoded_output)\n",
    "        break\n",
    "\n",
    "\n",
    "all_ds = train_ds.concatenate(test_ds).shuffle(config.BUFFER_SIZE).map(preprocess_text).batch(config.BATCH_SIZE)\n",
    "show_example(all_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ec00414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 360)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 360, 128)     3840000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 360, 128)     0           word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/multiheadattention (M (None, 360, 128)     66048       tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_dropout (Dropout) (None, 360, 128)     0           encoder_0/multiheadattention[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 360, 128)     0           tf.__operators__.add[0][0]       \n",
      "                                                                 encoder_0/att_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_layernormalizatio (None, 360, 128)     256         tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn (Sequential)      (None, 360, 128)     33024       encoder_0/att_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_dropout (Dropout) (None, 360, 128)     0           encoder_0/ffn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 360, 128)     0           encoder_0/att_layernormalization[\n",
      "                                                                 encoder_0/ffn_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_layernormalizatio (None, 360, 128)     256         tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mlm_cls (Dense)                 (None, 360, 30000)   3870000     encoder_0/ffn_layernormalization[\n",
      "==================================================================================================\n",
      "Total params: 7,809,584\n",
      "Trainable params: 7,809,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 360)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 360, 128)     3840000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 360, 128)     0           word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/multiheadattention (M (None, 360, 128)     66048       tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_dropout (Dropout) (None, 360, 128)     0           encoder_0/multiheadattention[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 360, 128)     0           tf.__operators__.add[0][0]       \n",
      "                                                                 encoder_0/att_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_layernormalizatio (None, 360, 128)     256         tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn (Sequential)      (None, 360, 128)     33024       encoder_0/att_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_dropout (Dropout) (None, 360, 128)     0           encoder_0/ffn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 360, 128)     0           encoder_0/att_layernormalization[\n",
      "                                                                 encoder_0/ffn_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_layernormalizatio (None, 360, 128)     256         tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mlm_cls (Dense)                 (None, 360, 30000)   3870000     encoder_0/ffn_layernormalization[\n",
      "==================================================================================================\n",
      "Total params: 7,809,584\n",
      "Trainable params: 7,809,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def bert_module(query, key, value, i):\n",
    "    # Multi headed self-attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "        name=f\"encoder_{i}/multiheadattention\")(query, key, value)\n",
    "    \n",
    "    attention_output = layers.Dropout(config.DROPOUT, name=f\"encoder_{i}/att_dropout\")(attention_output)\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=f\"encoder_{i}/att_layernormalization\")(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential([\n",
    "        layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "        layers.Dense(config.EMBED_DIM),\n",
    "    ], name=f\"encoder_{i}/ffn\")\n",
    "    \n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(config.DROPOUT, name=f\"encoder_{i}/ffn_dropout\")(ffn_output)\n",
    "    sequence_output = layers.LayerNormalization(epsilon=1e-6, name=f\"encoder_{i}/ffn_layernormalization\")(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "\n",
    "def get_pos_encoding_matrix(max_len, d_emb):\n",
    "    pos_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
    "            if pos != 0\n",
    "            else np.zeros(d_emb)\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(tf.keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        if len(inputs) == 3:\n",
    "            features, labels, sample_weight = inputs\n",
    "        else:\n",
    "            features, labels = inputs\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker]\n",
    "\n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    position = tf.range(start=0, limit=config.MAX_LEN, delta=1, dtype=tf.int32)\n",
    "    word_embeddings = layers.Embedding(config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\")(inputs)\n",
    "    position_embeddings = layers.Embedding(\n",
    "        input_dim=config.MAX_LEN,\n",
    "        output_dim=config.EMBED_DIM,\n",
    "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
    "        name=\"position_embedding\")(position)\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "    # this will go crazy for some reason\n",
    "    # embeddings = layers.Add(name='combine_embedding')([position_embeddings, word_embeddings])\n",
    "\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
    "        encoder_output\n",
    "    )\n",
    "    mlm_model = keras.Model(inputs, mlm_output, name='bert_model')\n",
    "    model_to_train = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR, epsilon=config.EPSILON)\n",
    "    model_to_train.compile(\n",
    "        optimizer=optimizer,\n",
    "        metrics=[tf.keras.metrics.Mean(name=\"loss\")])\n",
    "    return mlm_model, model_to_train\n",
    "\n",
    "\n",
    "keras.backend.clear_session()\n",
    "bert_model, bert_masked_model = create_masked_language_bert_model()\n",
    "bert_model.summary()\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0884ad9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 448s 286ms/step - loss: 6.8039\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'the',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as the singer here .',\n",
      " 'probability': 0.059381038}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': '.',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as . singer here .',\n",
      " 'probability': 0.056060143}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': ',',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as , singer here .',\n",
      " 'probability': 0.041484542}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'of',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as of singer here .',\n",
      " 'probability': 0.029813675}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as a singer here .',\n",
      " 'probability': 0.02894757}\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 450s 288ms/step - loss: 6.4934\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': '.',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as . singer here .',\n",
      " 'probability': 0.098537356}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'the',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as the singer here .',\n",
      " 'probability': 0.047265958}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': ',',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as , singer here .',\n",
      " 'probability': 0.03143169}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'and',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as and singer here .',\n",
      " 'probability': 0.027458783}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as a singer here .',\n",
      " 'probability': 0.025989795}\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 450s 288ms/step - loss: 6.2243\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': '.',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as . singer here .',\n",
      " 'probability': 0.07506042}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'the',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as the singer here .',\n",
      " 'probability': 0.061151244}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': ',',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as , singer here .',\n",
      " 'probability': 0.04068319}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as a singer here .',\n",
      " 'probability': 0.038513687}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'and',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as and singer here .',\n",
      " 'probability': 0.031098194}\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 443s 283ms/step - loss: 6.0431\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': '.',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as . singer here .',\n",
      " 'probability': 0.06900232}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'the',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as the singer here .',\n",
      " 'probability': 0.05354204}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as a singer here .',\n",
      " 'probability': 0.030377792}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': ',',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as , singer here .',\n",
      " 'probability': 0.024487898}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'of',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as of singer here .',\n",
      " 'probability': 0.024439337}\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 451s 289ms/step - loss: 5.8187\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'the',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as the singer here .',\n",
      " 'probability': 0.07047659}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': '.',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as . singer here .',\n",
      " 'probability': 0.04859626}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as a singer here .',\n",
      " 'probability': 0.041959904}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'and',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as and singer here .',\n",
      " 'probability': 0.019054359}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': ',',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as , singer here .',\n",
      " 'probability': 0.017623872}\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 440s 282ms/step - loss: 5.6013\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'the',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as the singer here .',\n",
      " 'probability': 0.11944608}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as a singer here .',\n",
      " 'probability': 0.07721416}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'and',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as and singer here .',\n",
      " 'probability': 0.022771617}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': ',',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as , singer here .',\n",
      " 'probability': 0.017220907}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': '.',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as . singer here .',\n",
      " 'probability': 0.016640788}\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 437s 280ms/step - loss: 5.4549\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'the',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as the singer here .',\n",
      " 'probability': 0.18154894}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as a singer here .',\n",
      " 'probability': 0.09834863}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': \"'\",\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               \", himesh is impressive as ' singer here .\",\n",
      " 'probability': 0.029160045}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'and',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as and singer here .',\n",
      " 'probability': 0.026845409}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': ',',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as , singer here .',\n",
      " 'probability': 0.019417971}\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 438s 280ms/step - loss: 5.3357\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'the',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as the singer here .',\n",
      " 'probability': 0.10907582}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as a singer here .',\n",
      " 'probability': 0.0876512}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'and',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as and singer here .',\n",
      " 'probability': 0.02410731}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': ',',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as , singer here .',\n",
      " 'probability': 0.020024696}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': '.',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as . singer here .',\n",
      " 'probability': 0.019175332}\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 440s 281ms/step - loss: 5.2473\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as a singer here .',\n",
      " 'probability': 0.20222126}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'the',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as the singer here .',\n",
      " 'probability': 0.08554858}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'an',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as an singer here .',\n",
      " 'probability': 0.021621363}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'and',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as and singer here .',\n",
      " 'probability': 0.01994719}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as this singer here .',\n",
      " 'probability': 0.019032752}\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 437s 280ms/step - loss: 5.1773\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as a singer here .',\n",
      " 'probability': 0.13018072}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'the',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as the singer here .',\n",
      " 'probability': 0.09001426}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': '.',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as . singer here .',\n",
      " 'probability': 0.027284265}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'and',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as and singer here .',\n",
      " 'probability': 0.023069033}\n",
      "{'input_text': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as [MASK] singer here .',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'rest star cast was simply okay . music and all songs are good '\n",
      "               ', himesh is impressive as this singer here .',\n",
      " 'probability': 0.02243447}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4870140760>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=5):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \" \".join([tokenizer.id_to_token(id) for id in ids if id != 0])\n",
    "    \n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        return tokenizer.id_to_token(id)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        masked_index = np.where(self.sample_tokens == MASK_TOKEN_ID)\n",
    "        masked_index = masked_index[1]\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "        \n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = values[i]\n",
    "            tokens = np.copy(self.sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            result = {\n",
    "                \"input_text\": self.decode(self.sample_tokens[0]),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)\n",
    "\n",
    "\n",
    "sample_text = 'rest star cast was simply okay. music and all songs are good, himesh is impressive as [MASK] singer here.'\n",
    "encoded_sample = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "\n",
    "generator_callback = MaskedTextGenerator(np.array([encoded_sample.ids]))\n",
    "bert_masked_model.fit(all_ds, epochs=config.EPOCHS, callbacks=[generator_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "646c2cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "bert_model.save(\"bert_mlm_imdb.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf3d18f",
   "metadata": {},
   "source": [
    "## Training Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dae755cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 360)\n",
      "(32,)\n",
      "is this the movie??? is this what indians are trying to show?? i think this is one more effort from a sick - minded director to turn down pakistani soldiers and in fact country.... but what we pakistani's know that we are always ahead of india in every part of our lives... not only in armed counters. < br / > < br / > well... this is bad filmed as that of border in early 1997... and director and writer just tried to overcome a shame of defeat in kargil by pakistani armed forces, by creating films like these.. < br / > < br / > one thing is very clear... whenever there will be an encounter between pakistan and india.... we will win....!!! so mr. dutta try to make some good movies instead of nonsense movies like this\n",
      "(32, 360)\n",
      "(32,)\n",
      "i went to see the movie because my boyfriend was raving about how much he wanted to see it, and how his friends had already been and loved it. so i came in with a neutral attitude, not really expecting the worst. unfortunately, that is what i got. i could write a 15 page paper on why this is easily the worst movie i have ever seen. but for your benefit i will point out the pros and the endless supply of cons. to begin, the acting was very good, especially christopher waltz and melanie laurent. there were also a few lines that deserved a laugh, and a couple suspenseful scenes. sadly, good acting, a little humor and suspense is where the good points end. the whole beginning story could have been great. a jewish girl's family is butchered, she goes into hiding, later encounters the man responsible for her pain and then hatches a great plan for revenge. sounds very good. but the movie wouldn't be quite as \" satisfying \" for the americans and jews craving nazi blood. i have no problem with wwii movies or killing nazis ; i saw defiance and like it very much. however, it's the way and attitude with which our would - be heroes kill. i'll give you a prime example of the kind of hypocrisy this movie oozes. our hoodlum gang ambushes a german unit and kills / scalps all of them except a sergeant and two other soldiers ( no problem so far ). they ask the sergeant to divulge information on another german unit. when he \" respectfully refuses \" to betray the lives of his fellow soldiers they bash his head in a with a baseball bat, cheering, and swearing as if they were at a football game. they took that nazi's head off yaaayy! now let's look at\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(entry):\n",
    "    def tokenize(tensor):\n",
    "        text = tensor.numpy().decode('utf-8')\n",
    "        result = tokenizer.encode(text, add_special_tokens=True)\n",
    "        ids = np.array(result.ids)\n",
    "        return tf.constant(ids, tf.int32)\n",
    "    \n",
    "    token_tensor = tf.py_function(tokenize, [entry['text']], Tout=tf.int32)\n",
    "    return token_tensor, entry['label']\n",
    "\n",
    "\n",
    "def show_downstream_task_example(ds):\n",
    "    for entry in ds:\n",
    "        x, y = entry\n",
    "        decoded_x = tokenizer.decode(x[0])\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        print(decoded_x)\n",
    "        break\n",
    "\n",
    "\n",
    "tokenized_train_ds = train_ds.shuffle(config.BUFFER_SIZE).map(tokenize_text).batch(config.BATCH_SIZE)\n",
    "tokenized_test_ds = test_ds.shuffle(config.BUFFER_SIZE).map(tokenize_text).batch(config.BATCH_SIZE)\n",
    "show_downstream_task_example(tokenized_train_ds)\n",
    "show_downstream_task_example(tokenized_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50a75533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"downstream_classifier_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 360)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 360, 128)     3840000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 360, 128)     0           word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/multiheadattention (M (None, 360, 128)     66048       tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_dropout (Dropout) (None, 360, 128)     0           encoder_0/multiheadattention[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 360, 128)     0           tf.__operators__.add[0][0]       \n",
      "                                                                 encoder_0/att_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_layernormalizatio (None, 360, 128)     256         tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn (Sequential)      (None, 360, 128)     33024       encoder_0/att_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_dropout (Dropout) (None, 360, 128)     0           encoder_0/ffn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 360, 128)     0           encoder_0/att_layernormalization[\n",
      "                                                                 encoder_0/ffn_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_layernormalizatio (None, 360, 128)     256         tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           encoder_0/ffn_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           8256        global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            65          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 3,947,905\n",
      "Trainable params: 3,947,905\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_downstream_classifier_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int32)\n",
    "    \n",
    "    pretrained_model = keras.models.load_model(\"bert_mlm_imdb.h5\")\n",
    "    \n",
    "    pretrained_target = pretrained_model(inputs)\n",
    "    sequence_output = pretrained_model.get_layer(\"encoder_0/ffn_layernormalization\").output\n",
    "    \n",
    "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
    "    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "    \n",
    "    downstream_model = tf.keras.Model(\n",
    "        pretrained_model.input,\n",
    "        outputs,\n",
    "        name='downstream_classifier_model')\n",
    "    return downstream_model, pretrained_model\n",
    "\n",
    "\n",
    "keras.backend.clear_session()\n",
    "downstream_model, pretrained = create_downstream_classifier_model()\n",
    "downstream_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c73cb7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 81s 103ms/step - loss: 0.7235 - accuracy: 0.5480 - val_loss: 0.7047 - val_accuracy: 0.5584\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 80s 102ms/step - loss: 0.6847 - accuracy: 0.5870 - val_loss: 0.6623 - val_accuracy: 0.6053\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 81s 103ms/step - loss: 0.6719 - accuracy: 0.5931 - val_loss: 0.6761 - val_accuracy: 0.5824\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.6652 - accuracy: 0.6020 - val_loss: 0.6922 - val_accuracy: 0.5620\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 80s 102ms/step - loss: 0.6606 - accuracy: 0.6053 - val_loss: 0.6483 - val_accuracy: 0.6253\n",
      "Epoch 1/5\n",
      "782/782 [==============================] - 78s 98ms/step - loss: 0.5210 - accuracy: 0.7352 - val_loss: 0.4128 - val_accuracy: 0.8089\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 77s 98ms/step - loss: 0.3237 - accuracy: 0.8618 - val_loss: 0.3539 - val_accuracy: 0.8463\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 75s 95ms/step - loss: 0.1749 - accuracy: 0.9311 - val_loss: 0.3852 - val_accuracy: 0.8470\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 77s 98ms/step - loss: 0.0697 - accuracy: 0.9754 - val_loss: 0.5071 - val_accuracy: 0.8425\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 77s 99ms/step - loss: 0.0280 - accuracy: 0.9901 - val_loss: 0.6167 - val_accuracy: 0.8390\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47b40c5280>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "# Train the classifier with frozen BERT stage\n",
    "pretrained.trainable = False\n",
    "downstream_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "downstream_model.fit(\n",
    "    tokenized_train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=tokenized_test_ds)\n",
    "\n",
    "\n",
    "# Unfreeze the BERT model for fine-tuning\n",
    "pretrained.trainable = True\n",
    "downstream_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "downstream_model.fit(\n",
    "    tokenized_train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=tokenized_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065fe15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
