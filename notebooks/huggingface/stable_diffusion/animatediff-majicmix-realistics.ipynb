{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074f61fe-8dd1-4fb1-a59a-fd33aa3cba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "/home/kiddos/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "home = os.getenv('HOME')\n",
    "p = StableDiffusionPipeline.from_single_file(path.join(home, 'civitai', 'animatediff', 'majicmixRealistic_v7.safetensors'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c894a79c-3855-4bd5-8f5a-a627692fc191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'motion_activation_fn': 'geglu', 'motion_attention_bias': False, 'motion_cross_attention_dim': None} were passed to MotionAdapter, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82efc8711da4f038723507ceea9085a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'transformer_layers_per_block': 1, 'attention_head_dim': 8, 'class_embed_type': None, 'addition_embed_type': None, 'addition_time_embed_dim': None, 'upcast_attention': None, 'projection_class_embeddings_input_dim': None} were passed to UNetMotionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import MotionAdapter, AnimateDiffPipeline\n",
    "from diffusers.schedulers import DPMSolverSinglestepScheduler\n",
    "from diffusers.utils import export_to_gif\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "adapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\")\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = AnimateDiffPipeline.from_pretrained(\n",
    "  model_id,\n",
    "  vae=p.vae.to(torch.float16),\n",
    "  unet=p.unet.to(torch.float16),\n",
    "  tokenizer=p.tokenizer,\n",
    "  motion_adapter=adapter,\n",
    "  torch_dtype=torch.float16,\n",
    "  use_safetensors=True,\n",
    ")\n",
    "scheduler = DPMSolverSinglestepScheduler.from_config(\n",
    "  pipe.scheduler.config\n",
    ")\n",
    "pipe.scheduler = scheduler\n",
    "\n",
    "# pipe.enable_sequential_cpu_offload()\n",
    "pipe.enable_vae_slicing()\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6305d777-2364-4b47-9c3e-91490a6753a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (101 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', award winning glamour photograph, modelshoot style, facing viewer, from font, looking at viewer,,, studio lighting,']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8c4c24486a4a4b9715dfedb0146e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/tmp/animatediff.gif'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = '1girl, solo, cowboy shot, latex bodysuit, huge breasts, sagging breasts, pink nipples, 8K, masterpiece, best quality, ultra high res, highres, extremely detailed, extremely delicate and beautiful, cinematic lighting, studio lighting, mottled light and shadow, hourglass figure, pronounced feminine feature, busty, narrow waist, elegant glamor pose, award winning glamour photograph, modelshoot style, facing viewer, from font, looking at viewer,, , studio lighting,'\n",
    "negative_prompt = 'makeup, lipstick,  loli, worst quality, lowres, watermark, deformed, distorted, disfigured, poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, (mutated hands and fingers:1.4), disconnected limbs, mutation, mutated, ugly, disgusting, amputation'\n",
    "\n",
    "generator = torch.Generator(\"cpu\").manual_seed(410293809141)\n",
    "output = pipe(\n",
    "  prompt=prompt.strip(),\n",
    "  negative_prompt=negative_prompt.strip(),\n",
    "  num_frames=16,\n",
    "  guidance_scale=7.0,\n",
    "  num_inference_steps=30,\n",
    "  generator=generator,\n",
    "  clip_skip=2,\n",
    ")\n",
    "frames = output.frames[0]\n",
    "export_to_gif(frames, \"/tmp/animatediff.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeffbcdf-7060-47ea-aeeb-b0ffecf4401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['leaves scattered among the flowers, a bittersweet atmosphere, a moment of quiet contemplation, soft and warm color palette, delicate brushwork, evocative use of light and shadow, subtle details in the wilting flowers, high contrast, color contrast ), masterpiece, official art, best quality, high quality, highres, natural light, ray tracing, volumetric light, realistic, photorealistic, ultra highres, vivid, nostalgia, bokeh,']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca66d44c132a416bb81029be4bed4ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/tmp/animatediff2.gif'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "1girl,leogirl,tsurime,solo,(smiling,happy),(bright red lips,blush:0.8),huge breasts,(sagging breasts),braids,walking,pose,\n",
    "(dreamflower,multicolored_background,A melancholic autumn scene in a vast flower field,a gentle breeze rustling through the dry grass,fallen leaves scattered among the flowers,a bittersweet atmosphere,a moment of quiet contemplation,Soft and warm color palette,delicate brushwork,evocative use of light and shadow,subtle details in the wilting flowers,high contrast,color contrast),\n",
    "masterpiece,official art,best quality,high quality,highres,natural light,ray tracing,volumetric light,realistic,photorealistic,ultra highres,vivid,nostalgia,bokeh,\n",
    "\"\"\"\n",
    "negative_prompt = \"\"\"\n",
    "BeyondV3-neg,BadNegAnatomyV1-neg,NegfeetV2,ng_deepnegative_v1_75t,EasyNegative,badhandv4,rev2-badprompt,verybadimagenegative_v1.3,bhands-neg,bad anatomy,bad hands,watermark,text,(worst quality,low quality,normal quality:1.5),lowres,((monochrome)),((grayscale)),username,(2 girls,multiple girls),negative_hand-neg, mutated arm, mutated hand, incorrect anatomy\n",
    "\"\"\"\n",
    "\n",
    "output = pipe(\n",
    "  prompt=prompt.strip(),\n",
    "  negative_prompt=negative_prompt.strip(),\n",
    "  num_frames=16,\n",
    "  guidance_scale=7.0,\n",
    "  num_inference_steps=30,\n",
    "  generator=generator,\n",
    "  clip_skip=2,\n",
    ")\n",
    "frames = output.frames[0]\n",
    "export_to_gif(frames, \"/tmp/animatediff2.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66542803-f5fc-4510-b1f0-d713ffaf6886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
