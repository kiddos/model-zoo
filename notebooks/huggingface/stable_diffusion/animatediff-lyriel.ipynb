{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a65b95-ba7f-4902-bb09-a9d3bb936d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "/home/kiddos/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "home = os.getenv('HOME')\n",
    "p = StableDiffusionPipeline.from_single_file(path.join(home, 'civitai', 'animatediff', 'lyriel_v16.safetensors'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b986413-563e-441c-b78a-602176d0f758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'motion_activation_fn': 'geglu', 'motion_attention_bias': False, 'motion_cross_attention_dim': None} were passed to MotionAdapter, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114a9af70f4a43acb39c0916480e000b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'transformer_layers_per_block': 1, 'attention_head_dim': 8, 'class_embed_type': None, 'addition_embed_type': None, 'addition_time_embed_dim': None, 'upcast_attention': None, 'projection_class_embeddings_input_dim': None} were passed to UNetMotionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler\n",
    "from diffusers.utils import export_to_gif\n",
    "\n",
    "adapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\")\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = AnimateDiffPipeline.from_pretrained(\n",
    "  model_id,\n",
    "  vae=p.vae.to(torch.float16),\n",
    "  unet=p.unet.to(torch.float16),\n",
    "  tokenizer=p.tokenizer,\n",
    "  motion_adapter=adapter,\n",
    "  torch_dtype=torch.float16,\n",
    "  use_safetensors=True,\n",
    ")\n",
    "scheduler = DDIMScheduler.from_config(\n",
    "  p.scheduler.config\n",
    ")\n",
    "pipe.scheduler = scheduler\n",
    "\n",
    "# pipe.enable_sequential_cpu_offload()\n",
    "pipe.enable_vae_slicing()\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e327866e-5d3a-426c-81f6-f90963df23ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (186 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['by greg rutkowski and artgerm, soft cinematic light, adobe lightroom, photolab, hdr, intricate, highly detailed, ( depth of field : 1. 4 ), faded, ( neutral colors : 1. 2 ), ( hdr : 1. 4 ), ( muted colors : 1. 2 ), hyperdetailed, ( artstation : 1. 4 ), cinematic, warm lights, dramatic light, ( intricate details : 1. 1 ), complex background, ( rutkowski : 0. 6 6 ), ( teal and orange : 0. 4 )']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00edba842de84f5e928a86b8e1cee41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/tmp/animatediff.gif'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = '(dark shot:1.1), epic realistic, portrait of halo, sunglasses, blue eyes, tartan scarf, white hair by atey ghailan, by greg rutkowski, by greg tocchini, by james gilleard, by joe fenton, by kaethe butcher, gradient yellow, black, brown and magenta color scheme, grunge aesthetic!!! graffiti tag wall background, art by greg rutkowski and artgerm, soft cinematic light, adobe lightroom, photolab, hdr, intricate, highly detailed, (depth of field:1.4), faded, (neutral colors:1.2), (hdr:1.4), (muted colors:1.2), hyperdetailed, (artstation:1.4), cinematic, warm lights, dramatic light, (intricate details:1.1), complex background, (rutkowski:0.66), (teal and orange:0.4)'\n",
    "negative_prompt = '3d, cartoon, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, artist name, young, loli, elf, 3d, illustration'\n",
    "\n",
    "generator = torch.Generator(\"cpu\").manual_seed(1261263585)\n",
    "output = pipe(\n",
    "  prompt=prompt.strip(),\n",
    "  num_frames=16,\n",
    "  guidance_scale=9.0,\n",
    "  num_inference_steps=30,\n",
    "  generator=generator,\n",
    "  clip_skip=2,\n",
    ")\n",
    "frames = output.frames[0]\n",
    "export_to_gif(frames, \"/tmp/animatediff.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2aac38e-d5e2-4605-bbb5-43b783fdba27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5195d19515844ce9a376160f65f5c861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/tmp/animatediff2.gif'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'a forbidden castle high up in the mountains, pixel art, intricate detail'\n",
    "negative_prompt = '(worst quality, low quality, letterboxed)'\n",
    "\n",
    "output = pipe(\n",
    "  prompt=prompt.strip(),\n",
    "  num_frames=16,\n",
    "  guidance_scale=9.0,\n",
    "  num_inference_steps=30,\n",
    "  clip_skip=2,\n",
    ")\n",
    "frames = output.frames[0]\n",
    "export_to_gif(frames, \"/tmp/animatediff2.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4dab67-52f7-41fb-ac98-687694981336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
