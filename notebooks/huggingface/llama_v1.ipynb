{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947d1995-4649-438a-a395-01b5e0786af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.31.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb14c9c3961d458f9366e4ef93b1f41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "print(transformers.__version__)\n",
    "\n",
    "model_id = 'decapoda-research/llama-7b-hf'\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    " load_in_4bit=True,\n",
    " bnb_4bit_quant_type='nf4',\n",
    " bnb_4bit_use_double_quant=True,\n",
    " bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model_nf4 = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id,\n",
    "  quantization_config=nf4_config,\n",
    "  trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b685c2-9bb6-4612-8d31-8e429b1557b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3796381696\n"
     ]
    }
   ],
   "source": [
    "print(model_nf4.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e7578a-5357-4e09-94da-dc84e82c6a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_nf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46da2ff3-6773-4f94-872c-ee5b000bbe1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "pipeline = pipeline(\n",
    "  'text-generation',\n",
    "  model=model_nf4,\n",
    "  tokenizer=tokenizer,\n",
    "  torch_dtype=torch.float16,\n",
    "  trust_remote_code=True,\n",
    "  device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2bb08d-a235-4205-8d1b-d7bc64281507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiddos/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A dialog, where User interacts with AI. AI is helpful, kind, obedient, honest, and knows its own limits. \n",
      "User: Hello, AI.\n",
      "AI: Hello! How can I assist you today?\n",
      "User: How much does this new dress cost?\n",
      "AI: 23.63\n",
      "User: Thanks. I think I'm going to buy a dress, a purse, and a hat.\n",
      "AI: You seem to have made up your mind, but you may need some help deciding. 1. Which is more important to you: style or comfort?\n",
      "2. How do you feel about your appearance?\n",
      "User: I'm very vain but I also think clothes should be practical.\n",
      "User: I think it's important for girls to have good-quality clothes, but I also care about being comfortable.\n",
      "AI: I understand. 1. Which do you prefer?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "A dialog, where User interacts with AI. AI is helpful, kind, obedient, honest, and knows its own limits. \n",
    "User: Hello, AI.\n",
    "AI: Hello! How can I assist you today?\n",
    "\"\"\"\n",
    "\n",
    "output = pipeline(\n",
    "  prompt,\n",
    "  max_length=200,\n",
    "  do_sample=True,\n",
    "  top_k=10,\n",
    "  num_return_sequences=1,\n",
    "  eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c581b7-2da3-43c4-893a-1024000775d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
