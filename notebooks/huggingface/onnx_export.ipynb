{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa0bf36-c918-405b-8e58-fd2b5cf3ea31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "usage: Hugging Face Transformers ONNX exporter [-h] -m MODEL\n",
      "                                               [--feature FEATURE]\n",
      "                                               [--opset OPSET] [--atol ATOL]\n",
      "                                               [--framework {pt,tf}]\n",
      "                                               [--cache_dir CACHE_DIR]\n",
      "                                               [--preprocessor {auto,tokenizer,feature_extractor,processor}]\n",
      "                                               [--export_with_transformers]\n",
      "                                               output\n",
      "\n",
      "positional arguments:\n",
      "  output                Path indicating where to store generated ONNX model.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -m MODEL, --model MODEL\n",
      "                        Model ID on huggingface.co or path on disk to load\n",
      "                        model from.\n",
      "  --feature FEATURE     The type of features to export the model with.\n",
      "  --opset OPSET         ONNX opset version to export the model with.\n",
      "  --atol ATOL           Absolute difference tolerance when validating the\n",
      "                        model.\n",
      "  --framework {pt,tf}   The framework to use for the ONNX export. If not\n",
      "                        provided, will attempt to use the local checkpoint's\n",
      "                        original framework or what is available in the\n",
      "                        environment.\n",
      "  --cache_dir CACHE_DIR\n",
      "                        Path indicating where to store cache.\n",
      "  --preprocessor {auto,tokenizer,feature_extractor,processor}\n",
      "                        Which type of preprocessor to use. 'auto' tries to\n",
      "                        automatically detect it.\n",
      "  --export_with_transformers\n",
      "                        Whether to use transformers.onnx instead of\n",
      "                        optimum.exporters.onnx to perform the ONNX export. It\n",
      "                        can be useful when exporting a model supported in\n",
      "                        transformers but not in optimum, otherwise it is not\n",
      "                        recommended.\n"
     ]
    }
   ],
   "source": [
    "!python3 -m transformers.onnx --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c001cf-c224-44b1-96c7-422438a5430e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Using framework PyTorch: 1.13.0a0+git49444c3\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/home/kiddos/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:794: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size <= 0:\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model output names match reference model ({'last_hidden_state'})\n",
      "\t- Validating ONNX Model output \"last_hidden_state\":\n",
      "\t\t-[✓] (3, 9, 768) matches (3, 9, 768)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "All good, model saved at: onnx/gpt2/gpt2.onnx/model.onnx\n",
      "/home/kiddos/.local/lib/python3.8/site-packages/transformers/onnx/__main__.py:178: FutureWarning: The export was done by transformers.onnx which is deprecated and will be removed in v5. We recommend using optimum.exporters.onnx in future. You can find more information here: https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!python3 -m transformers.onnx -m gpt2 --framework=pt onnx/gpt2/gpt2.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2aa33f4-84e7-404a-9dce-5e3f2a0df49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using framework PyTorch: 1.13.0a0+git49444c3\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model output names match reference model ({'last_hidden_state'})\n",
      "\t- Validating ONNX Model output \"last_hidden_state\":\n",
      "\t\t-[✓] (3, 9, 768) matches (3, 9, 768)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "All good, model saved at: onnx/bert/bert-base-cased.onnx/model.onnx\n",
      "/home/kiddos/.local/lib/python3.8/site-packages/transformers/onnx/__main__.py:178: FutureWarning: The export was done by transformers.onnx which is deprecated and will be removed in v5. We recommend using optimum.exporters.onnx in future. You can find more information here: https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!python3 -m transformers.onnx -m bert-base-cased --framework=pt onnx/bert/bert-base-cased.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd35c187-bdfd-4b33-ac67-21033619fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "usage: optimum-cli <command> [<args>] export onnx [-h] -m MODEL [--task TASK]\n",
      "                                                  [--for-ort]\n",
      "                                                  [--device DEVICE]\n",
      "                                                  [--opset OPSET]\n",
      "                                                  [--atol ATOL]\n",
      "                                                  [--framework {pt,tf}]\n",
      "                                                  [--pad_token_id PAD_TOKEN_ID]\n",
      "                                                  [--cache_dir CACHE_DIR]\n",
      "                                                  [--trust-remote-code]\n",
      "                                                  [--batch_size BATCH_SIZE]\n",
      "                                                  [--sequence_length SEQUENCE_LENGTH]\n",
      "                                                  [--num_choices NUM_CHOICES]\n",
      "                                                  [--width WIDTH]\n",
      "                                                  [--height HEIGHT]\n",
      "                                                  [--num_channels NUM_CHANNELS]\n",
      "                                                  [--feature_size FEATURE_SIZE]\n",
      "                                                  [--nb_max_frames NB_MAX_FRAMES]\n",
      "                                                  [--audio_sequence_length AUDIO_SEQUENCE_LENGTH]\n",
      "                                                  output\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "Required arguments:\n",
      "  -m MODEL, --model MODEL\n",
      "                        Model ID on huggingface.co or path on disk to load\n",
      "                        model from.\n",
      "  output                Path indicating the directory where to store generated\n",
      "                        ONNX model.\n",
      "\n",
      "Optional arguments:\n",
      "  --task TASK           The task to export the model for. If not specified,\n",
      "                        the task will be auto-inferred based on the model.\n",
      "                        Available tasks depend on the model, but are among:\n",
      "                        ['default', 'masked-lm', 'causal-lm', 'seq2seq-lm',\n",
      "                        'sequence-classification', 'token-classification',\n",
      "                        'multiple-choice', 'object-detection', 'question-\n",
      "                        answering', 'image-classification', 'image-\n",
      "                        segmentation', 'masked-im', 'semantic-segmentation',\n",
      "                        'speech2seq-lm', 'audio-classification', 'audio-frame-\n",
      "                        classification', 'audio-ctc', 'audio-xvector',\n",
      "                        'stable-diffusion']. For decoder models, use `xxx-\n",
      "                        with-past` to export the model using past key values\n",
      "                        in the decoder.\n",
      "  --for-ort             This exports models ready to be run with Optimum's\n",
      "                        ORTModel. Useful for encoder-decoder models\n",
      "                        forconditional generation. If enabled the encoder and\n",
      "                        decoder of the model are exported separately.\n",
      "  --device DEVICE       The device to use to do the export. Defaults to \"cpu\".\n",
      "  --opset OPSET         If specified, ONNX opset version to export the model\n",
      "                        with. Otherwise, the default opset will be used.\n",
      "  --atol ATOL           If specified, the absolute difference tolerance when\n",
      "                        validating the model. Otherwise, the default atol for\n",
      "                        the model will be used.\n",
      "  --framework {pt,tf}   The framework to use for the ONNX export. If not\n",
      "                        provided, will attempt to use the local checkpoint's\n",
      "                        original framework or what is available in the\n",
      "                        environment.\n",
      "  --pad_token_id PAD_TOKEN_ID\n",
      "                        This is needed by some models, for some tasks. If not\n",
      "                        provided, will attempt to use the tokenizer to guess\n",
      "                        it.\n",
      "  --cache_dir CACHE_DIR\n",
      "                        Path indicating where to store cache.\n",
      "  --trust-remote-code   Allow to use custom code for the modeling hosted in\n",
      "                        the model repository. This option should only be set\n",
      "                        for repositories you trust and in which you have read\n",
      "                        the code, as it will execute on your local machine\n",
      "                        arbitrary code present in the model repository.\n",
      "\n",
      "Input shapes (if necessary, this allows to override the shapes of the input given to the ONNX exporter, that requires an example input.):\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Text tasks only. Batch size to use in the example\n",
      "                        input given to the ONNX export.\n",
      "  --sequence_length SEQUENCE_LENGTH\n",
      "                        Text tasks only. Sequence length to use in the example\n",
      "                        input given to the ONNX export.\n",
      "  --num_choices NUM_CHOICES\n",
      "                        Text tasks only. Num choices to use in the example\n",
      "                        input given to the ONNX export.\n",
      "  --width WIDTH         Image tasks only. Width to use in the example input\n",
      "                        given to the ONNX export.\n",
      "  --height HEIGHT       Image tasks only. Height to use in the example input\n",
      "                        given to the ONNX export.\n",
      "  --num_channels NUM_CHANNELS\n",
      "                        Image tasks only. Number of channels to use in the\n",
      "                        example input given to the ONNX export.\n",
      "  --feature_size FEATURE_SIZE\n",
      "                        Audio tasks only. Feature size to use in the example\n",
      "                        input given to the ONNX export.\n",
      "  --nb_max_frames NB_MAX_FRAMES\n",
      "                        Audio tasks only. Maximum number of frames to use in\n",
      "                        the example input given to the ONNX export.\n",
      "  --audio_sequence_length AUDIO_SEQUENCE_LENGTH\n",
      "                        Audio tasks only. Audio sequence length to use in the\n",
      "                        example input given to the ONNX export.\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f82499d3-768e-45e1-a03f-d41fb23ba534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Using framework PyTorch: 1.13.0a0+git49444c3\n",
      "Overriding 2 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "\t- pad_token_id -> 0\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model output names match reference model (logits)\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 16, 50257) matches (2, 16, 50257)\n",
      "\t\t-[x] values not close enough, max diff: 0.0004425048828125 (atol: 1e-05)\n",
      "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 1e-05:\n",
      "- logits: max diff = 0.0004425048828125.\n",
      " The exported model was saved at: gpt2_onnx\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx --task=causal-lm --framework=pt --model gpt2 gpt2_onnx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40ce53-34c8-4b2c-bbf9-3b3f6b1a9703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
