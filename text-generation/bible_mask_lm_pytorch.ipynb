{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b115efe1-d572-46be-9ac7-fba29b1bd8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bibles (/home/kiddos/huggingface/datasets/versae___bibles/testament/1.0.0/896439b9ad0fc0fb6df27a9146d8823e457a75f06e1a90169728cb690a531248)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013838529586791992,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe806802a64344c9aeb7ddd36bf15236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'language', 'year', 'century', 'codebook'],\n",
       "        num_rows: 1570633\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'language', 'year', 'century', 'codebook'],\n",
       "        num_rows: 216493\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'language', 'year', 'century', 'codebook'],\n",
       "        num_rows: 216511\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('versae/bibles', 'testament')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ec11e80-0e9a-46af-a4e0-f1c99f2823e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kiddos/huggingface/datasets/versae___bibles/testament/1.0.0/896439b9ad0fc0fb6df27a9146d8823e457a75f06e1a90169728cb690a531248/cache-e0bbf6e18a061e00.arrow\n",
      "Loading cached processed dataset at /home/kiddos/huggingface/datasets/versae___bibles/testament/1.0.0/896439b9ad0fc0fb6df27a9146d8823e457a75f06e1a90169728cb690a531248/cache-c4a4fdb95491d310.arrow\n",
      "Loading cached processed dataset at /home/kiddos/huggingface/datasets/versae___bibles/testament/1.0.0/896439b9ad0fc0fb6df27a9146d8823e457a75f06e1a90169728cb690a531248/cache-039fcd18d6c43b3d.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'language', 'year', 'century', 'codebook'],\n",
       "        num_rows: 26226\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'language', 'year', 'century', 'codebook'],\n",
       "        num_rows: 3618\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'language', 'year', 'century', 'codebook'],\n",
       "        num_rows: 3619\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"language\"] == 'CHI')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94756187-702b-4283-b517-298e1ea20a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '但 现 在 你 们 要 弃 绝 这 一 切 的 事 ， 以 及 恼 恨 、 忿 怒 、 恶 毒 （ 或 作 ： 阴 毒 ） 、 毁 谤 ， 并 口 中 污 秽 的 言 语 。',\n",
       " 'label': 0,\n",
       " 'language': 'CHI',\n",
       " 'year': 0,\n",
       " 'century': 'unk',\n",
       " 'codebook': 'COL'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f76019-7d25-46bf-9b24-647ffdc418f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 20:08:27.519134: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'bert-base-chinese'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd2e89ed-ba83-49b3-989f-b0f38ea19c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/kiddos/huggingface/datasets/versae___bibles/testament/1.0.0/896439b9ad0fc0fb6df27a9146d8823e457a75f06e1a90169728cb690a531248/cache-1d69f3cde902eea6.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/kiddos/huggingface/datasets/versae___bibles/testament/1.0.0/896439b9ad0fc0fb6df27a9146d8823e457a75f06e1a90169728cb690a531248/cache-b3c2f925ea0f3340.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/kiddos/huggingface/datasets/versae___bibles/testament/1.0.0/896439b9ad0fc0fb6df27a9146d8823e457a75f06e1a90169728cb690a531248/cache-938addf39aaf4c02.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 26226\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3618\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3619\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'].replace(' ', ''))\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, remove_columns=['text', 'label', 'language', 'year', 'century', 'codebook']\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37623b82-25be-4dae-9f8e-74cfd385321f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/kiddos/huggingface/datasets/versae___bibles/testament/1.0.0/896439b9ad0fc0fb6df27a9146d8823e457a75f06e1a90169728cb690a531248/cache-6ba0069df21beb6b.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/kiddos/huggingface/datasets/versae___bibles/testament/1.0.0/896439b9ad0fc0fb6df27a9146d8823e457a75f06e1a90169728cb690a531248/cache-a245d557c770a4c5.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/kiddos/huggingface/datasets/versae___bibles/testament/1.0.0/896439b9ad0fc0fb6df27a9146d8823e457a75f06e1a90169728cb690a531248/cache-029e7a268329ce94.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3825\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 523\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 533\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 256\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    result = {}\n",
    "    for k, t in concatenated_examples.items():\n",
    "        result[k] = [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets = lm_datasets.remove_columns(['token_type_ids'])\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d931a311-7120-4bba-b314-54b27ac41e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 但 现 在 你 们 要 弃 绝 这 一 切 的 事 ， 以 及 恼 恨 、 忿 怒 、 恶 毒 （ 或 作 ： 阴 毒 ） 、 毁 谤 ， 并 口 中 污 秽 的 言 语 。 [SEP] [CLS] 說 話 的 時 候 、 有 一 個 法 利 賽 人 請 耶 穌 同 他 喫 飯 ． 耶 穌 就 進 去 坐 席 。 [SEP] [CLS] 虽 然 是 这 样 ， 这 些 醉 生 梦 死 的 人 还 是 照 样 玷 污 身 体 ， 藐 视 主 权 ， 毁 谤 尊 荣 。 [SEP] [CLS] 在 他 纳 和 米 吉 多 ， 并 靠 近 撒 拉 他 拿 、 耶 斯 列 下 边 的 伯 善 全 地 ， 从 伯 善 到 亚 伯 米 何 拉 直 到 约 念 之 外 ， 冇 亚 希 律 的 儿 子 巴 拿 ； [SEP] [CLS] 以 後 再 沒 有 咒 詛 ． 在 城 裡 有 神 和 羔 羊 的 寶 座 ． 他 的 僕 人 都 要 事 奉 他 ． [SEP] [CLS] 因 為 他 沒 有 甚 麼 償 還 之 物 、 主 人 吩 咐 把 他 和 他 妻 子 兒 女 、 並 一 切 所 有 的 都 賣 了 償 還 。 [SEP] [CLS] 约 坦 登 基 的 时 候 ， 是 二 十 五 岁 ； 他 在 耶'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b44cc65-2ed7-457d-8314-40704f5aafea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 但 现 在 你 们 要 弃 绝 这 一 切 的 事 ， 以 及 恼 恨 、 忿 怒 、 恶 毒 （ 或 作 ： 阴 毒 ） 、 毁 谤 ， 并 口 中 污 秽 的 言 语 。 [SEP] [CLS] 說 話 的 時 候 、 有 一 個 法 利 賽 人 請 耶 穌 同 他 喫 飯 ． 耶 穌 就 進 去 坐 席 。 [SEP] [CLS] 虽 然 是 这 样 ， 这 些 醉 生 梦 死 的 人 还 是 照 样 玷 污 身 体 ， 藐 视 主 权 ， 毁 谤 尊 荣 。 [SEP] [CLS] 在 他 纳 和 米 吉 多 ， 并 靠 近 撒 拉 他 拿 、 耶 斯 列 下 边 的 伯 善 全 地 ， 从 伯 善 到 亚 伯 米 何 拉 直 到 约 念 之 外 ， 冇 亚 希 律 的 儿 子 巴 拿 ； [SEP] [CLS] 以 後 再 沒 有 咒 詛 ． 在 城 裡 有 神 和 羔 羊 的 寶 座 ． 他 的 僕 人 都 要 事 奉 他 ． [SEP] [CLS] 因 為 他 沒 有 甚 麼 償 還 之 物 、 主 人 吩 咐 把 他 和 他 妻 子 兒 女 、 並 一 切 所 有 的 都 賣 了 償 還 。 [SEP] [CLS] 约 坦 登 基 的 时 候 ， 是 二 十 五 岁 ； 他 在 耶'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets['train'][0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd849f06-42ce-4e46-8257-1f2ff84b1c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n",
      "torch.Size([8, 256])\n",
      "torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "train_dataloader = DataLoader(\n",
    "    lm_datasets['train'],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    lm_datasets['validation'],\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "for example_batch in train_dataloader:\n",
    "    break\n",
    "\n",
    "print(example_batch['input_ids'].shape)\n",
    "print(example_batch['attention_mask'].shape)\n",
    "print(example_batch['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68b2c3d8-4a77-4123-bd3f-c469c64bcf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3221,  6905,  6121,  2345,  2769,  3341,  5442,  4638,  3192,   103,\n",
      "          103,   976,   103,   800,  4638,  2339,   511,   102,   101,  4310,\n",
      "         1922,   782,  1348,  2897,  6629,  4767,  1928,   103,  2802,   103,\n",
      "          511,   102,   101, 16449,  3341,  6963,  3221,   855,  1146,  1920,\n",
      "         4638,  5314,   855,  1146,  2207,  4638,  4867,  4886,  8024,  6821,\n",
      "          103,  3690,   103,  4542,  7309,  4638,   511,   102,   101,   679,\n",
      "         6814,  8024,   872,   103,  6206,   103,  2127,  2347,  5307,  2533,\n",
      "         4708,  4638,  8024,   103,  1168,   103,  3341,   511,   102,   101,\n",
      "         5635,   754,  2626,  3471,  8024,   800,  4638,  2797,  3667,  3221,\n",
      "          103,  2626,  4638,  8024,   800,  1745,  6450,  2626,   103,  8024,\n",
      "         4500,  5994,   969,  4638,   103,   103,  3673,  4127,  1737,  5736,\n",
      "         4638,   782,  8024,  1315,   886,  1762,   103,   726,   103,  6382,\n",
      "         6596,  4415,  4638,  3198,   952,  8024,   800,   738,  3221,   103,\n",
      "         3416,   511,   102,   101,  2400,   684,  2533,  4708,   872,   812,\n",
      "          928,  2552,  4638,  3362,  3126,  8024,   103,  3221,  4130,  7789,\n",
      "         4638,   103,  2617,   511,   102,   101,   852,  1081,   671,   816,\n",
      "          752,  8024,   103,  1403,   872,  2824,  6371,  8024,  2218,  3221,\n",
      "          800,   103,  2792,  4917,   103,  2460,  4999,  4638,  6887,  8024,\n",
      "         2769,  3633,  2902,  4708,  6929,   103,   752,  1938,  2769,   103,\n",
      "         2134,  4638,  4868,  8024,  1348,   928,  1394,   103,  2526,  3791,\n",
      "         4638,  1469,  1044,  4761, 13304,   103,   671,  1147,  2792,  6381,\n",
      "          103,   103,  8024,   102,   101,   800,   947,  5481,  6210,  4374,\n",
      "         4638,  6282,   510,  2218,  1343,   749,  8026,  1762,  3346,   103,\n",
      "         2792,  4692,  6210,  4638,  6929,  3215,   103,  2575,  4197,  1762,\n",
      "          800,   947,  1184,  7531,  6121,   510,  4684, 10776,  1168,  2207,\n",
      "         2111,  2094,  4638,  1765,  9163,   103])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, 2692, 8024, -100,\n",
      "        2768, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, 6206, -100,  800, -100, -100, -100, 1403, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, 3221, -100, 3187, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100,  812, -100, 2898, -100, -100, -100, -100, -100, -100,\n",
      "        -100, 4684, -100, 2769, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, 6932, -100, -100, -100, -100, -100,\n",
      "        -100, 2626, 6369, -100, -100, -100, -100, -100, 6241, 6427, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, 4956, -100,  782, -100,\n",
      "        1062, -100, -100, -100, -100, -100, -100, -100, -100, 6821, -100, -100,\n",
      "        -100, -100, -100,  684, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, 2218, -100, -100, -100, -100, 3131, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, 2769, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100,  812, -100, -100,  711, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, 6887, -100, -100, -100, 4862, -100, -100,\n",
      "        -100, -100, 1348, -100, -100,  725, -100, -100, -100, -100, -100, -100,\n",
      "         741,  677, -100, -100, -100, -100, 6770, 4638, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, 3175, -100, -100, -100, -100, -100, -100,  510, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, 6121, -100, -100, -100, -100,\n",
      "        -100, -100, 3175,  510])\n",
      "是 遵 行 差 我 来 者 的 旨 [MASK] [MASK] 做 [MASK] 他 的 工 。 [SEP] [CLS] 犹 太 人 又 拿 起 石 头 [MASK] 打 [MASK] 。 [SEP] [CLS]柯 来 都 是 位 分 大 的 给 位 分 小 的 祝 福 ， 这 [MASK] 毫 [MASK] 疑 问 的 。 [SEP] [CLS] 不 过 ， 你 [MASK] 要 [MASK] 守 已 经 得 着 的 ， [MASK] 到 [MASK] 来 。 [SEP] [CLS] 至 于 恶 棍 ， 他 的 手 段 是 [MASK] 恶 的 ， 他 图 谋 恶 [MASK] ， 用 虚 假 的 [MASK] [MASK] 毁 灭 困 苦 的 人 ， 即 使 在 [MASK] 乏 [MASK] 讲 赂 理 的 时 候 ， 他 也 是 [MASK] 样 。 [SEP] [CLS] 并 且 得 着 你 们 信 心 的 果 效 ， [MASK] 是 灵 魂 的 [MASK] 恩 。 [SEP] [CLS] 但 冇 一 件 事 ， [MASK] 向 你 承 认 ， 就 是 他 [MASK] 所 称 [MASK] 异 端 的 道 ， 我 正 按 着 那 [MASK] 事 奉 我 [MASK] 宗 的 神 ， 又 信 合 [MASK] 律 法 的 和 先 知ルハイト [MASK] 一 切 所 记 [MASK] [MASK] ， [SEP] [CLS] 他 們 聽 見 王 的 話 、 就 去 了 ． 在 東 [MASK] 所 看 見 的 那 星 [MASK] 忽 然 在 他 們 前 頭 行 、 直 1～2 到 小 孩 子 的 地７ [MASK]\n"
     ]
    }
   ],
   "source": [
    "print(example_batch['input_ids'][0])\n",
    "print(example_batch['labels'][0])\n",
    "print(tokenizer.decode(example_batch['input_ids'][0]))\n",
    "# print(tokenizer.decode(example_batch['labels'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f5a524f-07df-4d30-a236-5fa5d8781451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "learning_rate = 1.25e-5\n",
    "accelerator = Accelerator()\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bcb0b86-4492-474b-a5af-5c85ad589983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.6577, device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def run_evaluation():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    try:\n",
    "        perplexity = torch.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f7f6d89-0543-479a-9a02-f961023a9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "epochs = 10\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b79d99db-7119-4a9c-a94a-21170ebffe9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010416746139526367,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4790,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1263ff630c3042c484b5d287c92366c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0: Perplexity: 4.772298336029053\n",
      ">>> Epoch 1: Perplexity: 4.474167823791504\n",
      ">>> Epoch 2: Perplexity: 4.149725914001465\n",
      ">>> Epoch 3: Perplexity: 4.04140567779541\n",
      ">>> Epoch 4: Perplexity: 3.9288315773010254\n",
      ">>> Epoch 5: Perplexity: 3.920513868331909\n",
      ">>> Epoch 6: Perplexity: 3.9138827323913574\n",
      ">>> Epoch 7: Perplexity: 3.7478549480438232\n",
      ">>> Epoch 8: Perplexity: 3.6737732887268066\n",
      ">>> Epoch 9: Perplexity: 3.689364194869995\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "output_dir = f'{model_name}-finetuned-bible-mlm'\n",
    "\n",
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        perplexity = run_evaluation()\n",
    "        print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "        # Save and upload\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4209e40-d051-4cf0-87e7-68bbaeac19ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model.to('cpu')\n",
    "fill_masker = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "398447d1-d1c7-40d4-a6ce-bfae9098c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1923,  6432,  8038,  5023,   103,  2094,  3171,   749,  1959,  8024,\n",
      "         2769,   912,  2372,   103,   677,   103,  3308,  6224,  5456,   103,\n",
      "         1290,  8024,   886,   800,   103,   103,   857,  1762,  6929,  7027,\n",
      "          511,   102,   101,  5456,  4947,  1726,  5031,  6303,   510,  2769,\n",
      "          738,  6206,  1558,   103,   103,   671,  1368,  6282,  8026,   872,\n",
      "          947,   684,  1440,  6260,  2769,  8026,   102,   101,   103,  1728,\n",
      "         4881,   103,  8024,   103,   103,  6763,  2483,   103,  1213,   103,\n",
      "         2769,  4638,  6716,   860,  3867,  4607,   103,  2397,   511,   102,\n",
      "          101,  1728,  3634,  8024,  6832,  6206,  2650,   103,  8024,  1762,\n",
      "          677,  4638,  1921,   738,  2553,  7946,  3266,  8039,  1728,   711,\n",
      "         2769,   103,   103,  1139,  8024,  2769,  2692,  2347,  2137,  8024,\n",
      "         2553,   679,  3204,  2637,  8024,   738,   679,  6760,  2692,   679,\n",
      "          976,   511,   102,   101,  2769,   738,  4761,  6887,   510,  1762,\n",
      "        17610,  6174,  7531,   510,  2218,  3221,  2769,  5489,   103,   722,\n",
      "          704,   510,  3760,  3300,  5679,   103,  8026,  1728,  4158,   103,\n",
      "         2562,  4158,  1587,  4507,  2533,  2769,   510,  1372,  3221,   103,\n",
      "         1139,   103,  4507,   679,  2533,  2769,   511,   102,   101,   712,\n",
      "         2190,  2769,  1963,  3634,  6432,  8038, 17904,   103,  6392,  4989,\n",
      "         2127,  3307,   103,  8024,   103,   103,  2199,  2792,  4692,  6224,\n",
      "         4638,  6835,  6432,   103,   102,   101,  1728,   103,  2769,   812,\n",
      "         4638,  4868,   718,  3221,  4164,  4125,   511,   102,   101,   103,\n",
      "         3300,  2823,  1168,   103,  2218,  6760,  1726,  5456,  6662,  3054,\n",
      "         1107,  2823,   800,   511,   102,   101,  4692,  1525,  8024,   103,\n",
      "          812,  6845,  6912,  4135,  7410,  8039,  1812,  1350,   782,  2553,\n",
      "         3119,  3657,   800,   103,  4638,  2221,  7674,  8024,   103,  2472,\n",
      "          782,  2553,  5623,  1813,   800,   812], device='cuda:0')\n",
      "镰 刀 。 [SEP] [CLS] [MASK] 色 列 人 早 晨 起 [MASK] 、 對 著 基 比 亞 安 營 。 [SEP] [CLS] 他 们 用 [MASK] 斗 衡 量 的 时 候 ， 多 [MASK] 的 没 有 剩 余 ， 少 收 的 也 不 缺 胴 ； 各 [MASK] 按 着 自 己 的 食 量tors 取 。 [SEP] [CLS] 亚 撒 把 他 父 亲 所 分 [MASK] 为 [MASK] [MASK] [MASK] [MASK] 己 所 分 别 为 [MASK] 的 金 银 和 器妹 ， 都 [MASK] 到 耶 和 [MASK] 的 殿 里 。 [SEP] [CLS] 帐 幕 拆 [MASK] ， 革 顺妙 子 孙 和 [MASK] 拉 利 的 子 孙 就 抬 着 帐 [MASK] 先 往 前 delete 。 [SEP] [CLS] 不 要 轻 率 [MASK] 开 王 的 面 前 ， [MASK] 不 要 [MASK] 与 [MASK] 事 ， 因 为 王 可 [MASK] 随 己 意 [MASK] 任 何 事 。 [SEP] [CLS] 他 们 不 [MASK] 别 人 受 苦 ， 也 [MASK] [MASK] 别 人 [MASK] 灾 。 [SEP] [CLS] [MASK] [MASK] 卷 走 你 所 有 的 [MASK] [MASK] ， 你 [MASK] 盟 友 要 被 掳 去 ； 那 时 ， 因 [MASK] 的 一 切 恶 行 ， 你 必 蒙 羞 受 辱 。 [SEP] [CLS] 因 此 [MASK] 我 用 比 喻 对 他 们 讲 话 [MASK] [MASK] [MASK] 他 [MASK] 看 却 看 不 见 ， 听 也 [MASK] 不 到 ， 也\n"
     ]
    }
   ],
   "source": [
    "for test_batch in eval_dataloader:\n",
    "    break\n",
    "\n",
    "print(test_batch['input_ids'][-1])\n",
    "print(tokenizer.decode(example_batch['input_ids'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7975f81-2eb0-4683-b361-afff833c6750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.971808135509491,\n",
       "   'token': 4933,\n",
       "   'token_str': '稣',\n",
       "   'sequence': '[CLS] 耶 稣 回 答 ： [UNK] [MASK] 神 [MASK] 差 来 的 [SEP]'},\n",
       "  {'score': 0.008269641548395157,\n",
       "   'token': 2787,\n",
       "   'token_str': '户',\n",
       "   'sequence': '[CLS] 耶 户 回 答 ： [UNK] [MASK] 神 [MASK] 差 来 的 [SEP]'},\n",
       "  {'score': 0.004375677090138197,\n",
       "   'token': 4947,\n",
       "   'token_str': '穌',\n",
       "   'sequence': '[CLS] 耶 穌 回 答 ： [UNK] [MASK] 神 [MASK] 差 来 的 [SEP]'},\n",
       "  {'score': 0.00393082806840539,\n",
       "   'token': 6662,\n",
       "   'token_str': '路',\n",
       "   'sequence': '[CLS] 耶 路 回 答 ： [UNK] [MASK] 神 [MASK] 差 来 的 [SEP]'},\n",
       "  {'score': 0.0015573027776554227,\n",
       "   'token': 1469,\n",
       "   'token_str': '和',\n",
       "   'sequence': '[CLS] 耶 和 回 答 ： [UNK] [MASK] 神 [MASK] 差 来 的 [SEP]'}],\n",
       " [{'score': 0.6156852841377258,\n",
       "   'token': 794,\n",
       "   'token_str': '从',\n",
       "   'sequence': '[CLS] 耶 [MASK] 回 答 ： [UNK] 从 神 [MASK] 差 来 的 [SEP]'},\n",
       "  {'score': 0.32717201113700867,\n",
       "   'token': 3221,\n",
       "   'token_str': '是',\n",
       "   'sequence': '[CLS] 耶 [MASK] 回 答 ： [UNK] 是 神 [MASK] 差 来 的 [SEP]'},\n",
       "  {'score': 0.00950069073587656,\n",
       "   'token': 2769,\n",
       "   'token_str': '我',\n",
       "   'sequence': '[CLS] 耶 [MASK] 回 答 ： [UNK] 我 神 [MASK] 差 来 的 [SEP]'},\n",
       "  {'score': 0.00488457502797246,\n",
       "   'token': 872,\n",
       "   'token_str': '你',\n",
       "   'sequence': '[CLS] 耶 [MASK] 回 答 ： [UNK] 你 神 [MASK] 差 来 的 [SEP]'},\n",
       "  {'score': 0.004520535469055176,\n",
       "   'token': 1127,\n",
       "   'token_str': '凡',\n",
       "   'sequence': '[CLS] 耶 [MASK] 回 答 ： [UNK] 凡 神 [MASK] 差 来 的 [SEP]'}],\n",
       " [{'score': 0.6263510584831238,\n",
       "   'token': 2792,\n",
       "   'token_str': '所',\n",
       "   'sequence': '[CLS] 耶 [MASK] 回 答 ： [UNK] [MASK] 神 所 差 来 的 [SEP]'},\n",
       "  {'score': 0.12778200209140778,\n",
       "   'token': 4266,\n",
       "   'token_str': '父',\n",
       "   'sequence': '[CLS] 耶 [MASK] 回 答 ： [UNK] [MASK] 神 父 差 来 的 [SEP]'},\n",
       "  {'score': 0.019494175910949707,\n",
       "   'token': 886,\n",
       "   'token_str': '使',\n",
       "   'sequence': '[CLS] 耶 [MASK] 回 答 ： [UNK] [MASK] 神 使 差 来 的 [SEP]'},\n",
       "  {'score': 0.01523332018405199,\n",
       "   'token': 4130,\n",
       "   'token_str': '灵',\n",
       "   'sequence': '[CLS] 耶 [MASK] 回 答 ： [UNK] [MASK] 神 灵 差 来 的 [SEP]'},\n",
       "  {'score': 0.014024988748133183,\n",
       "   'token': 1760,\n",
       "   'token_str': '圣',\n",
       "   'sequence': '[CLS] 耶 [MASK] 回 答 ： [UNK] [MASK] 神 圣 差 来 的 [SEP]'}]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_masker('耶 [MASK] 回 答 ： [UNK] [MASK] 神 [MASK] 差 来 的')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9f8b886-a06d-4830-aa17-00da97ee31a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.8843076825141907,\n",
       "   'token': 6205,\n",
       "   'token_str': '西',\n",
       "   'sequence': '[CLS] 来 到 希 西 家 那 里 ， 将 拉 [MASK] [MASK] 基 的 话 [MASK] 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.025877993553876877,\n",
       "   'token': 5384,\n",
       "   'token_str': '罗',\n",
       "   'sequence': '[CLS] 来 到 希 罗 家 那 里 ， 将 拉 [MASK] [MASK] 基 的 话 [MASK] 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.020009057596325874,\n",
       "   'token': 1239,\n",
       "   'token_str': '勒',\n",
       "   'sequence': '[CLS] 来 到 希 勒 家 那 里 ， 将 拉 [MASK] [MASK] 基 的 话 [MASK] 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.013175214640796185,\n",
       "   'token': 2526,\n",
       "   'token_str': '律',\n",
       "   'sequence': '[CLS] 来 到 希 律 家 那 里 ， 将 拉 [MASK] [MASK] 基 的 话 [MASK] 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.011612769216299057,\n",
       "   'token': 1392,\n",
       "   'token_str': '各',\n",
       "   'sequence': '[CLS] 来 到 希 各 家 那 里 ， 将 拉 [MASK] [MASK] 基 的 话 [MASK] 诉 了 他 。 [SEP]'}],\n",
       " [{'score': 0.1964188665151596,\n",
       "   'token': 3683,\n",
       "   'token_str': '比',\n",
       "   'sequence': '[CLS] 来 到 希 [MASK] 家 那 里 ， 将 拉 比 [MASK] 基 的 话 [MASK] 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.09126697480678558,\n",
       "   'token': 843,\n",
       "   'token_str': '伯',\n",
       "   'sequence': '[CLS] 来 到 希 [MASK] 家 那 里 ， 将 拉 伯 [MASK] 基 的 话 [MASK] 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.04685242846608162,\n",
       "   'token': 4377,\n",
       "   'token_str': '玛',\n",
       "   'sequence': '[CLS] 来 到 希 [MASK] 家 那 里 ， 将 拉 玛 [MASK] 基 的 话 [MASK] 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.03710639849305153,\n",
       "   'token': 5310,\n",
       "   'token_str': '结',\n",
       "   'sequence': '[CLS] 来 到 希 [MASK] 家 那 里 ， 将 拉 结 [MASK] 基 的 话 [MASK] 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.03428928181529045,\n",
       "   'token': 6205,\n",
       "   'token_str': '西',\n",
       "   'sequence': '[CLS] 来 到 希 [MASK] 家 那 里 ， 将 拉 西 [MASK] 基 的 话 [MASK] 诉 了 他 。 [SEP]'}],\n",
       " [{'score': 0.5683292746543884,\n",
       "   'token': 2225,\n",
       "   'token_str': '尼',\n",
       "   'sequence': '[CLS] 来 到 希 [MASK] 家 那 里 ， 将 拉 [MASK] 尼 基 的 话 [MASK] 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.10116372257471085,\n",
       "   'token': 3172,\n",
       "   'token_str': '斯',\n",
       "   'sequence': '[CLS] 来 到 希 [MASK] 家 那 里 ， 将 拉 [MASK] 斯 基 的 话 [MASK] 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.03526022657752037,\n",
       "   'token': 7931,\n",
       "   'token_str': '麦',\n",
       "   'sequence': '[CLS] 来 到 希 [MASK] 家 那 里 ， 将 拉 [MASK] 麦 基 的 话 [MASK] 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.026656432077288628,\n",
       "   'token': 762,\n",
       "   'token_str': '亚',\n",
       "   'sequence': '[CLS] 来 到 希 [MASK] 家 那 里 ， 将 拉 [MASK] 亚 基 的 话 [MASK] 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.025531696155667305,\n",
       "   'token': 2861,\n",
       "   'token_str': '拉',\n",
       "   'sequence': '[CLS] 来 到 希 [MASK] 家 那 里 ， 将 拉 [MASK] 拉 基 的 话 [MASK] 诉 了 他 。 [SEP]'}],\n",
       " [{'score': 0.997098445892334,\n",
       "   'token': 1440,\n",
       "   'token_str': '告',\n",
       "   'sequence': '[CLS] 来 到 希 [MASK] 家 那 里 ， 将 拉 [MASK] [MASK] 基 的 话 告 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.0011792473960667849,\n",
       "   'token': 6760,\n",
       "   'token_str': '转',\n",
       "   'sequence': '[CLS] 来 到 希 [MASK] 家 那 里 ， 将 拉 [MASK] [MASK] 基 的 话 转 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.00019714758673217148,\n",
       "   'token': 2971,\n",
       "   'token_str': '控',\n",
       "   'sequence': '[CLS] 来 到 希 [MASK] 家 那 里 ， 将 拉 [MASK] [MASK] 基 的 话 控 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.0001900986535474658,\n",
       "   'token': 2832,\n",
       "   'token_str': '投',\n",
       "   'sequence': '[CLS] 来 到 希 [MASK] 家 那 里 ， 将 拉 [MASK] [MASK] 基 的 话 投 诉 了 他 。 [SEP]'},\n",
       "  {'score': 0.00017931328329723328,\n",
       "   'token': 2495,\n",
       "   'token_str': '归',\n",
       "   'sequence': '[CLS] 来 到 希 [MASK] 家 那 里 ， 将 拉 [MASK] [MASK] 基 的 话 归 诉 了 他 。 [SEP]'}]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_masker('来 到 希 [MASK] 家 那 里 ， 将 拉 [MASK] [MASK] 基 的 话 [MASK] 诉 了 他 。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343ac2e-abfb-4220-b787-c8785a3f3ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
